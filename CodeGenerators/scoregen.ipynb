{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re, json, os , logging , random, html, datetime \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from lib.config import connstr\n",
    "from lib.context import context\n",
    "from lib.utils import * \n",
    "from lib.db_parser import db_parser \n",
    "from lib.picklist_recommender import picklist_recommender \n",
    "from lib.question_type_recommender import question_type_recommender\n",
    "from lib.script_generator import script_generator   \n",
    "from lib.dbupdate_parser import dbupdate_parser\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "ctx=context() \n",
    "ctx.logger.setLevel(logging.DEBUG)\n",
    "config = {}\n",
    "with open('config.json', 'r') as f: \n",
    "    config=json.loads(f.read())    \n",
    "ctx.config=config \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ps=PorterStemmer()  \n",
    "sw=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=''\n",
    "for aspx in [f for f in listdir(r'C:\\dev\\CyberScope\\CyberScopeBranch\\CSwebdev\\code\\CyberScope\\HVA\\2023') if re.search('aspx$', f)]:  \n",
    "    s=''\n",
    "    fname=fr\"C:\\dev\\CyberScope\\CyberScopeBranch\\CSwebdev\\code\\CyberScope\\HVA\\2023\\{aspx}\"\n",
    "    with open(fname,encoding=\"utf8\", errors='ignore' ) as f:\n",
    "        all=f'{all}\\n <!-- path:{fname}  --> \\n{f.read()} '  \n",
    "with open(f\"{ctx.get_dest()}script.aspx\", 'w', encoding='UTF-8') as f:\n",
    "    f.write(all)\n",
    "\n",
    "def db_updates():\n",
    "    pkforms={}\n",
    "    script = ''\n",
    "    for filename in os.listdir(fr'C:\\dev\\CyberScope\\CyberScopeBranch\\CSwebdev\\database\\SA'):\n",
    "        f = os.path.join(fr'C:\\dev\\CyberScope\\CyberScopeBranch\\CSwebdev\\database\\SA', filename) \n",
    "        if not 'Import' in f: \n",
    "            sql=''\n",
    "            with open(f, 'r', encoding='utf-8') as file: \n",
    "                sql=file.read()\n",
    "                group = re.search(\"'(2023-A-\\w{3,})'\",sql)\n",
    "                pkforms[group.groups(1)[0]]=f \n",
    "                script=f'{script}\\n{\"--\"}path:{f}\\n {sql}' \n",
    "    with open(ctx.get_dest() + '\\dbupdates.sql', 'w', encoding='UTF-8') as file: \n",
    "        file.write(script)\n",
    "    return pkforms\n",
    "pkforms = db_updates()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in listdir(r'C:\\Users\\timko\\Downloads\\Questionnaire\\1.5') if re.search('xlsx', f)]   \n",
    "files = { \n",
    "'2023-A-HVAARC':'HVA Network Architecture Questionnaire.xlsx' ,\n",
    "'2023-A-HVAASSET':'HVA Asset Management Questionnaire.xlsx' ,\n",
    "'2023-A-HVABC':'HVA Business Continuity Management Questionnaire.xlsx' ,\n",
    "'2023-A-HVADPRO':'HVA Data Protection Questionnaire.xlsx',\n",
    "'2023-A-HVAENDPRO':'HVA Endpoint Protection Questionnaire.xlsx',\n",
    "'2023-A-HVAIDENT':'HVA Identity and Access Management Questionnaire.xlsx',\n",
    "'2023-A-HVAMD':'HVA Monitoring and Detection Questionnaire.xlsx',\n",
    "'2023-A-HVAREM':'HVA Remote Access Questionnaire.xlsx',\n",
    "'2023-A-HVAVULN':'HVA Vulnerability Management Questionnaire.xlsx',\n",
    "'2023-A-HVAAPPSEC':'HVA Application Security Questionnaire.xlsx'\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ufencoder (s, length=10): \n",
    "    s = ''.join([ps.stem(w) for w in s.split(' ') if w.lower() not in sw]) \n",
    "    s=s.upper().strip() \n",
    "    s=re.sub('[^A-Z0-9]','',s)\n",
    "    for c in 'IOU':\n",
    "        if len(s) > length:\n",
    "            s=re.sub(c,'',s) \n",
    "    if len(s) > length: \n",
    "        s=s[:7]+s[len(s)-3:]\n",
    "    return s[:length]\n",
    "    \n",
    "def qnormalize(s): \n",
    "    s=s.strip()\n",
    "    s=re.sub('[^A-Za-z0-9]','',str(s)) \n",
    "    return s\n",
    "def normalize(s): \n",
    "    s=s.upper().strip()\n",
    "    s=re.sub('[^A-Z0-9\\s]','',str(s))\n",
    "    s=re.sub('\\s{1,}','',str(s))\n",
    "    return s\n",
    "normalize('Jump host access (e.g., the user accesses the HVA from an intermediate host)Â ')\n",
    " \n",
    "def get_recommender(PK_FORM, normalizer=normalize):\n",
    "    pl=sql_todf(f\"\"\"\n",
    "    SELECT LTRIM(RTRIM(DisplayValue))  [Selections], PK_PickList [ML_Value], PK_PickListType [PK_PickListType]\n",
    "    FROM vwPickLists  \n",
    "    WHERE PK_PickListType  > 400  \n",
    "    ORDER BY PK_PickListType DESC\n",
    "    \"\"\", connstr)   \n",
    "    pl['SelectionsNorm'] = pl['Selections'].apply(normalize) \n",
    "    pr = picklist_recommender(ctx, picklist_where=f\" PK_PickListType  > 400 \") \n",
    " \n",
    "    return pr, pl\n",
    "\n",
    "def xl_to_df(PK_FORM, version=''): \n",
    "    path=r'C:\\Users\\timko\\Downloads\\Questionnaire\\\\'+version + files[PK_FORM]\n",
    "    print(PK_FORM + ' ' + path)\n",
    "    converts={\n",
    "        'Selections':lambda x: re.sub(\"^(\\s?\\d{,2}\\.)\",'',str(x)).strip() \n",
    "        , 'SML':lambda x: re.sub('[^A-Z0-9]','',str(x)).strip() \n",
    "        , 'ML Scored':lambda x: str(x).strip() \n",
    "        , 'Questions':lambda x: x.replace(0, None).strip() \n",
    "    }\n",
    "    df=pd.read_excel(f'{path}', converters=converts,  sheet_name=4, header=2, usecols='B:I, M:R').reset_index()  \n",
    "    \n",
    "    for c in ['ID','Question','SML']:  df[c] = df[c].replace(to_replace=[0, '0'], method='ffill')\n",
    "    for c in df.columns:  df = df.rename({c: re.sub('\\s','_',c) }, axis='columns')\n",
    "    \n",
    "    if 'ML' in df.columns:\n",
    "        df[\"ML_Scored\"]= df[\"ML_Score\"].apply(lambda x: str(x).strip())\n",
    "        df['ML_Score']=df[\"ML\"].apply(lambda x: str(x).strip())\n",
    "        df=df.drop(columns='ML')\n",
    " \n",
    "    df['PK_FORM']=PK_FORM\n",
    "    df[\"ID\"].fillna(method='ffill', inplace=True) \n",
    "    df['Question'].fillna(method='ffill', inplace=True) \n",
    "    df.drop(columns=['Check_Answer', 'index'], inplace=True)\n",
    "     \n",
    "    df = df.loc[ df[\"ID\"].str.contains('^\\w\\w\\d(\\w){0,5}$', na=False, regex=True) ]   \n",
    "    #df = df.loc[ df[\"SML\"].str.contains('^SML(\\d)$', na=False, regex=True) ]   \n",
    "    df=df.dropna(subset = ['Selections'])\n",
    "    \n",
    "    df['ML_Score'] = df['ML_Score'].apply(lambda s: re.sub('[^0-9\\.]','',str(s)))\n",
    "    df['Selections']=df['Selections'].apply(lambda s: str(s).replace(\"'\",'`'))\n",
    "    df['SelectionsNorm'] = df['Selections'].apply(normalize)\n",
    "    df['nQuestion'] = df['Question'].apply(qnormalize)\n",
    "    df['SML']=df['ML_Score'].apply(lambda s: f'SML{s}') \n",
    "    \n",
    "    df.fillna('', inplace=True) \n",
    "    for c in df.columns:  \n",
    "        df[c]=df[c].astype('str')    \n",
    "    return df\n",
    "\n",
    "def apply_picklists(df): \n",
    "    PK_FORM=df.iloc[0]['PK_FORM']\n",
    "    pr, pl=get_recommender(PK_FORM=PK_FORM, normalizer=normalize)\n",
    "    dfp=df.groupby(['ID'], as_index=False).agg({'SelectionsNorm':list})  \n",
    "    dfp['PK_PickListType']=dfp['SelectionsNorm'].apply(lambda s: pr.recommend(s, normalizer=normalize, threshhold=(.97, .93))['PK_PicklistType'])\n",
    "  \n",
    "    dff=pd.merge(df,dfp.loc[:,['ID','PK_PickListType']], how='left', left_on='ID', right_on='ID')\n",
    "    dff=pd.merge(dff,pl, how='left', left_on=['SelectionsNorm','PK_PickListType'], right_on=['SelectionsNorm','PK_PickListType']) \n",
    "    dff.rename(columns={'Selections_x': 'Selections'}, inplace=True)\n",
    "    dff.ML_Value=dff.ML_Value.fillna(0).astype('int32').astype('str') \n",
    "    dff['PK_Picklist']=dff.ML_Value.astype('int32')\n",
    "    dff = dff.drop(columns=[c for c in dff.columns if re.search('_\\w$', c) ])\n",
    "    dff['sortorder'] = range(1, len(dff)+1) \n",
    "    if 'ZT_Score' in dff.columns:\n",
    "        dff=dff.drop(columns=['ZT_Score'])\n",
    "    \n",
    "    return dff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_sheets():\n",
    "    dff=pd.DataFrame()\n",
    "    version='1.5'\n",
    "    for k in files.keys():\n",
    "        df = xl_to_df(k, version=version+'\\\\')\n",
    "        df = apply_picklists(df)\n",
    "        dff=pd.concat([df,dff]) \n",
    "        \n",
    "    dff.to_csv(ctx.get_dest() + f'all{version}.csv', index=False)\n",
    "    dff.to_html(ctx.get_dest() + f'all{version}.html', index=False)\n",
    "\n",
    "    grp=dff.groupby(['PK_FORM','ID','Question','PK_PickListType'], as_index=False).agg({'Selections': list}) \n",
    "    grp.sort_values(['PK_FORM'], inplace=True)\n",
    "    grp['Selections']=grp['Selections'].apply('|'.join)\n",
    "    grp.to_html(ctx.get_dest()+ f'all_new_questions{version}.html', escape=False, index=False)\n",
    "    grp.to_csv(ctx.get_dest() + f'all_new_questions{version}.csv', index=False)\n",
    "\n",
    "    dff=dff.loc[ dff[\"SML\"].str.contains('^SML(\\d)$', na=False, regex=True) ]   \n",
    "\n",
    "    dff.to_csv(ctx.get_dest() + f'scored{version}.csv', index=False)\n",
    "    dff.to_html(ctx.get_dest() + f'scored{version}.html', index=False)\n",
    "load_sheets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15=pd.read_csv(ctx.get_dest() + 'all_new_questions1.5.csv' )  \n",
    "df15['nQuestion']=df15.Question.apply(qnormalize)\n",
    "df10=pd.read_csv(ctx.get_dest() + 'all_new_questions1.0.csv' ) \n",
    "df10['nQuestion']=df10.Question.apply(qnormalize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df15[\n",
    "    df15['PK_FORM']=='2023-A-HVAMD'\n",
    "]\n",
    "df.sort_values('').to_html(ctx.get_dest() + '2023-A-HVAMD.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "merged = df10.merge( df15, left_on=[  'ID' ], right_on=[ 'ID' ], how='left', indicator=True, suffixes=('','_y'))\n",
    "merged=merged.loc[merged._merge=='left_only']\n",
    "merged=merged.loc[:, ['PK_FORM','ID' ,'Question','PK_PickListType', 'Selections']].drop_duplicates() \n",
    "\n",
    "print('|'.join(merged.ID)) \n",
    "db=db_parser(ctx).parse('4028:4259')\n",
    "db=db.loc[db['{idt}'].isin(list(merged.ID)) & db['{idt}'].str.contains('RA')]\n",
    "db=sort_df_alphanum(db, '{idt}')\n",
    "code=script_generator(ctx).generate(db)\n",
    "# print( code ) # did to ID,  todo EP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_picks():\n",
    "    plgen = script_generator(ctx) \n",
    "    all=pd.read_csv(ctx.get_dest() + 'all.csv' )  \n",
    "    dfp=all.loc[all.PK_PickListType==0 ]   \n",
    "    grp=dfp.groupby('ID', as_index=False).agg({'Selections': list}) \n",
    "    grp['PK_PLT'] = [i+600 for i in range(0,len(grp) )]  \n",
    "    grp['sql']=[plgen.list_to_sql(x, UsageField=y, PK_PickListType=z, PK_PickList=10470, Description=y, encoding=ufencoder)[1] for x,y,z in zip(grp['Selections'], grp['ID'], grp['PK_PLT'])]\n",
    "    grp['sql']=grp['sql'].apply(lambda s: s.replace('\\n','<br>'))\n",
    "    grp[['sql']].to_html(ctx.get_dest()+'\\\\newpicks.html', escape=False, index=False)  \n",
    "    return grp\n",
    "#new_picks = get_new_picks()\n",
    "#new_picks \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk=get_avail_pk(400, sql=\"SELECT DISTINCT PK_PickListType FROM vwPicklists\") \n",
    "# next(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_diff(): \n",
    "    dff = pd.read_csv(ctx.get_dest() + 'all_new_questions1.5.csv' )  \n",
    "    dff['nQuestion']=dff['Question'].apply(qnormalize)\n",
    "    \n",
    "    db=db_parser(ctx).parse('4028:4259') \n",
    "    db['nQuestion']=db['{QuestionText}'].apply(qnormalize) \n",
    "    db.to_html(ctx.get_dest() + 'db.html')\n",
    " \n",
    "    merged = dff.merge(db,  left_on=['nQuestion', 'ID' ], right_on=['nQuestion', '{idt}' ], how='left', indicator=True)\n",
    "    merged=merged.loc[merged._merge== 'left_only']\n",
    "    merged=merged.loc[:, ['PK_FORM','ID' ,'Question','PK_PickListType', 'Selections']].drop_duplicates()\n",
    "\n",
    "    merged['PK_QuestionType']=17  \n",
    "    merged['sql']=\"(_PK, @FormName, @PK_QGroup, 1, N'ID', PK_QuestionType, 1, PK_PickListType, 0,NULL,N'Question',NULL),\"\n",
    "    pk=get_avail_pk(42700) \n",
    "    qr = question_type_recommender(ctx, verbose=True, use_cache=True)\n",
    "    for i,r in merged.iterrows():     \n",
    "        merged.loc[i, 'PK_QuestionType']=qr.recommend(r['Question'])['PK_QuestionType']\n",
    "        merged.loc[i, 'sql'] = merged.loc[i, 'sql'].replace('_PK',str(next(pk))).replace('ID', merged.loc[i, 'ID']).replace('PK_PickListType', str(merged.loc[i, 'PK_PickListType'])).replace('PK_QuestionType', str(merged.loc[i, 'PK_QuestionType'])).replace('Question', merged.loc[i, 'Question'])\n",
    "  \n",
    "    return merged\n",
    "dbdiff=db_diff()\n",
    "dbdiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for pkform in [k for k in pkforms.keys() if k =='2023-A-HVAARC']:\n",
    "    print(pkform)\n",
    "    script = dbupdate_parser(ctx, path=pkforms[pkform], verbose=True)   \n",
    "    form = dbdiff.loc[dbdiff.PK_FORM == pkform ]\n",
    "    for i,r in form.iterrows(): \n",
    "        #script.update_question_text( r['ID'], r['Question'].replace(\"'\",\"''\") )  \n",
    "        insert = script.get_insert(r['ID'])\n",
    "    #script.inspect()\n",
    "    print(script.nf) \n",
    "dbdiff.loc[dbdiff['ID'].isin(script.nf)]['sql'].to_csv(ctx.get_dest() + 'sql.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = pd.read_csv(f'{ctx.get_dest()}scored1.5.csv').fillna('')\n",
    "scored = scored.drop(['nQuestion', 'Question', 'SelectionsNorm'], axis=1)\n",
    "scored['Selections'] = scored['Selections'].apply(lambda s: s[:10])\n",
    "cols=['PK_FORM','ID', 'SML', 'PK_Picklist'] \n",
    "for c in [c for c in scored.columns if not re.search( 'PK_FORM|SML|^ID$|PK_Picklist$|Basic_|Mod_|Adv_' ,c)]:\n",
    "    cols.append(c)\n",
    "print(cols)\n",
    "scored=scored[cols]\n",
    "ins, create, temp=SQL_INSERT_FROM_DF(scored, TABLE_NAME='HVASAScoring') \n",
    "script = '\\n'.join(ins)\n",
    "with open(f'{ctx.get_dest()}script.sql', 'w') as f: \n",
    "    f.write(f\"{create} \\n\\n {script}\"  )\n",
    "with open(r'C:\\dev\\CyberScope\\CyberScopeBranch\\CSwebdev\\database\\SA\\_101_DB_Update_SAScoring_Excel_Import.sql', 'w') as f: \n",
    "    f.write( f\"{create} \\n\\n {script}\"  )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a7c076192e1e16d3ee1a218e6831777c4d2e3a001a9a33f6decd6c7672cec63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
