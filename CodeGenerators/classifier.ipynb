{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re , datetime , logging, pickle, os\n",
    "from lib.config import connstr\n",
    "from lib.utils import * \n",
    "from lib.question_type_recommender import question_type_recommender\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "import shap \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lib.context import context\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ctx=context() \n",
    "ctx.logger.setLevel(logging.DEBUG)\n",
    "config = {}\n",
    "with open('config.json', 'r') as f: \n",
    "    config=json.loads(f.read())    \n",
    "ctx.config=config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = question_type_recommender(ctx, verbose=True, use_cache=True)\n",
    "qt=qr.recommend('Does the HVA or its supporting infrastructure network employ an intrusion detection system (IDS) or intrusion prevention system (IPS) as a perimeter defense? (Select all that apply.)')\n",
    "qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(18,4.7)}) \n",
    "plt.style.use('Solarize_Light2') # fivethirtyeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['YN','CNT','TXT','PICK','FREQ','DEC','MULTICHECKBOX' ]\n",
    "qt=sql_todf(f\"\"\"\n",
    "SELECT PK_QuestionTypeId, Code ,description FROM fsma_QuestionTypes\n",
    "  WHERE Code IN ('YN','CNT','YNA','TXT','PICK','LABEL','FREQ','DEC','MULTICHECKBOX')\n",
    "    \"\"\", connstr)   \n",
    "\n",
    "df=sql_todf(f\"\"\"\n",
    "    SELECT PK_Question, FK_QuestionType, Code, QuestionText \n",
    "    FROM fsma_Questions \n",
    "\t  INNER JOIN fsma_QuestionTypes ON fsma_Questions.FK_QuestionType=fsma_QuestionTypes.PK_QuestionTypeId\n",
    "\t  WHERE FK_QuestionType IS NOT NULL AND QuestionText IS NOT NULL  \n",
    "    AND Code IN ('{\"','\".join(codes)}')  \n",
    "    -- AND PK_Question < 40000\n",
    "    ORDER BY PK_QUESTION DESC\n",
    "    \"\"\", connstr)  \n",
    "\n",
    "def _normalizer(s):\n",
    "    s=re.sub('[^a-z0-9\\s\\-\\?]','',s.lower().strip() )\n",
    "    s=re.sub('\\s{2,}',' ',s)\n",
    "    return s \n",
    "_normalizer('How many threat model exercises [34] were conducted in the last reporting period ?')\n",
    "#qt\n",
    "# df[df['Code'].isin(['FREQ','DEC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df['Code'].value_counts().index, df['FK_QuestionType'].value_counts().values, alpha=0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df['QuestionText']=df['QuestionText'].apply(_normalizer)  \n",
    "model = LogisticRegression(max_iter=200) \n",
    "lod=[]\n",
    "for ng in [ (1,3), (1,4)   ]: # ,  , (1,3), (1,4) (1,2), \n",
    "    for i in range(1,3,1): # 0.002\n",
    "        start = datetime.datetime.now()\n",
    "        mindf = i*.001   \n",
    "        vectorizer = CountVectorizer(ngram_range=ng, min_df=mindf, analyzer='word',token_pattern=u'\\w+|\\?')\n",
    "        \n",
    "        X = vectorizer.fit_transform(df['QuestionText'])   \n",
    "        sparse_matrix = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X.toarray()) \n",
    "        sparse_matrix=pd.merge(df, sparse_matrix, left_index=True, right_index=True)  \n",
    "        sparse_matrix.drop(['QuestionText', 'PK_Question', 'Code' ], inplace=True, axis=1) \n",
    "        y = sparse_matrix['FK_QuestionType'].values \n",
    "        X = sparse_matrix.drop(['FK_QuestionType' ], axis=1).values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred=model.predict(X_test)\n",
    "        score=model.score(X_test, y_test) \n",
    "        stop = datetime.datetime.now()\n",
    "        report = classification_report(y_test, y_pred, target_names=codes)\n",
    "        d={\n",
    "            'score':score,\n",
    "            'time': (stop-start).seconds,\n",
    "            'ngram_range': ng,\n",
    "            'min_df': mindf,\n",
    "            'sparse_matrix.shape':sparse_matrix.shape,\n",
    "            'report':report\n",
    "        }\n",
    "        print(d)  \n",
    "        lod.append(d)\n",
    "scores = pd.DataFrame(lod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( scores.loc[0, 'report'] )  \n",
    "scores.sort_values(by='score')\n",
    "if len(scores) > 1:\n",
    "    sns.lineplot(data=scores, x='time', y='score' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params =  { \n",
    "    'max_iter': [50, 200, 1000]\n",
    "} \n",
    "grid = GridSearchCV(estimator=model,  param_grid=params, cv=10,  n_jobs=1) \n",
    "grid.fit(X_train, y_train) \n",
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = abs(model.coef_[0])\n",
    "coefficients[0]\n",
    "fi = pd.DataFrame({'Feature': sparse_matrix.columns[1:], 'Importance': coefficients})\n",
    "fi = fi.sort_values('Importance', ascending=False)   \n",
    "plot = sns.barplot(x=fi[:50].Feature , y=fi[:50].Importance) \n",
    "plt.xticks(rotation=90)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, X_test )\n",
    "shap_values = explainer.shap_values(X_test)   \n",
    "shap.summary_plot( shap_values,  X_test, max_display=25,  feature_names=sparse_matrix.columns[1:],  plot_size=[18,16] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1=sql_todf(f\"\"\"\n",
    "    SELECT  FK_QuestionType, Code, QuestionText \n",
    "    FROM fsma_Questions \n",
    "\t  INNER JOIN fsma_QuestionTypes ON fsma_Questions.FK_QuestionType=fsma_QuestionTypes.PK_QuestionTypeId\n",
    "\t  WHERE FK_QuestionType IS NOT NULL AND QuestionText IS NOT NULL  \n",
    "    AND Code IN ('YN', 'CNT', 'TXT', 'PICK', 'FREQ', 'DEC', 'MULTICHECKBOX') --  'YNA', 'FREQ','DEC',\n",
    "    AND PK_Question > 40000\n",
    "    ORDER BY PK_QUESTION DESC\n",
    "    \"\"\", connstr)\n",
    "print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lod=[]\n",
    "for i,r in df1.iterrows():\n",
    "    rec = qr.recommend(r['QuestionText'])\n",
    "    if str(r['FK_QuestionType']) != str(rec):\n",
    "        d={\n",
    "            'actual': r['FK_QuestionType'],\n",
    "            'rec': rec,\n",
    "            'QuestionText': r['QuestionText']\n",
    "        }\n",
    "        lod.append(d)\n",
    "dff=pd.DataFrame(lod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17 = dff.loc[dff.actual == 17]\n",
    "d17.to_csv('out/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=_normalizer('How many threat model exercises [34] were conducted in the last reporting period ?')\n",
    "print(txt)\n",
    "X = vectorizer.transform([txt])  \n",
    "prediction=model.predict(X.toarray())\n",
    "print ('\\nPK_QuestionType : '+ str(prediction[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred) \n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(codes))\n",
    "plt.xticks(tick_marks, codes)\n",
    "plt.yticks(tick_marks, codes) \n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, fmt='g' , xticklabels=codes, yticklabels=codes) \n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_predict(s,threshold=.25): \n",
    "    s=_normalizer(s)\n",
    "    print ( s )\n",
    "    dfff=df.copy()\n",
    "    for i,r in dfff.iterrows():  \n",
    "        normed = _normalizer(r['QuestionText'] )\n",
    "        ratio=SequenceMatcher(None, s, normed).ratio() \n",
    "        dfff.loc[i, 'ratio'] = ratio\n",
    "        #if ratio > threshold: break\n",
    "    d=dfff.loc[dfff.ratio > threshold]\n",
    "    d=d.sort_values(by=['ratio'], ascending=False) \n",
    "    return d\n",
    "dfp=sm_predict(\"\"\"\n",
    "\n",
    "   Is there an internet-facing method to access the system?\n",
    "    \"\"\")\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _normalizer(s):\n",
    "    s=re.sub('[^a-z0-9\\s\\-\\?]','',s.lower().strip() )\n",
    "    s=re.sub('\\s{2,}',' ',s)\n",
    "    s=' '.join([ ps.stem(s) for s in s.split(' ')])\n",
    "    return s \n",
    "_normalizer('Has the Agency implemented an automated capability to detect and block unauthorized hardware from connecting to the network?' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sql_todf(f\"\"\"\n",
    "    SELECT  QuestionText, CAST(FK_PickListType AS NVARCHAR(9)) Y -- FK_QuestionType, Code, \n",
    "    FROM fsma_Questions \n",
    "\t  INNER JOIN fsma_QuestionTypes ON fsma_Questions.FK_QuestionType=fsma_QuestionTypes.PK_QuestionTypeId\n",
    "\t  WHERE FK_QuestionType IS NOT NULL AND QuestionText IS NOT NULL  \n",
    "    AND Code IN ( 'PICK',  'MULTICHECKBOX') \n",
    "    AND FK_PickListType IN (124,157,162,168,330,423,85,88,89,97)\n",
    "    ORDER BY PK_QUESTION DESC\n",
    "    \"\"\", connstr)\n",
    "df['QuestionText']=df['QuestionText'].apply(_normalizer)\n",
    "df=df.sort_values('QuestionText').drop_duplicates(subset=['Y', 'QuestionText'], keep='last')\n",
    "pk1=df[:-500]\n",
    "pk2=df[500:]\n",
    "df.Y.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), min_df=0, stop_words='english', analyzer='word' ) \n",
    "X = vectorizer.fit_transform(df['QuestionText'])   \n",
    "vectorizer.get_feature_names() \n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X.toarray()) \n",
    "sparse_matrix=pd.merge(df, sparse_matrix, left_index=True, right_index=True)  \n",
    "sparse_matrix.drop(['QuestionText'], inplace=True, axis=1) \n",
    "y = sparse_matrix['Y'].values \n",
    "features = sparse_matrix.drop(['Y'], axis=1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model= LogisticRegression()\n",
    "# model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "score=model.score(X_test, y_test) \n",
    "report = classification_report(y_test, y_pred )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)\n",
    "print ( report ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=_normalizer(\"Did you evaluate iDP sources as described in the guidance? Did you harden SSO feature as described in the guidance? Did you turn on advanced logging and establish a PAM baseline(Expected privileged account state) for Cloud?\")\n",
    "print(txt)\n",
    "X = vectorizer.transform([txt])  \n",
    "prediction=model.predict(X.toarray())\n",
    "print ('\\nprediction: '+ str(prediction[0])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, X_test )\n",
    "shap_values = explainer.shap_values(X_test)   \n",
    "shap.summary_plot( shap_values,  X_test, max_display=25,  feature_names=sparse_matrix.columns[1:],  plot_size=[18,16] ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
