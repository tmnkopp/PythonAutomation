{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7abce19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20220506-py3-none-any.whl (5.6 MB)\n",
      "Collecting cryptography~=36.0.0\n",
      "  Downloading cryptography-36.0.2-cp36-abi3-win_amd64.whl (2.2 MB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\tim\\anaconda3\\lib\\site-packages (from cryptography~=36.0.0->pdfminer.six) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tim\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography~=36.0.0->pdfminer.six) (2.20)\n",
      "Installing collected packages: cryptography, charset-normalizer, pdfminer.six\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 3.4.7\n",
      "    Uninstalling cryptography-3.4.7:\n",
      "      Successfully uninstalled cryptography-3.4.7\n",
      "Successfully installed charset-normalizer-2.0.12 cryptography-36.0.2 pdfminer.six-20220506\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02170fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217b2ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32511ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"FY 2021  \\n\\nInspector GeneraI \\n\\nFederal Information \\n\\nSecurity Modernization Act of 2014  \\n\\n(FISMA) Reporting Metrics \\n\\nVersion 1.1 \\n\\nMay 12, 2021 \\n\\nPage 1 of 60 \\n\\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nDocument History \\n\\nVersion \\n\\nDate \\n\\nComments \\n\\n1.0 \\n\\n1.0 \\n\\n02/19/2021 \\n\\nInitial draft \\n\\nVarious \\n\\nUpdated criteria references throughout the metrics \\n\\nSec/Page \\n\\nAll \\n\\nAll \\n\\n1.0 \\n\\nVarious \\n\\nAdded a Frequently Asked Question (FAQ) Section \\n\\nPg. 10 \\n\\n1.0 \\n\\nVarious \\n\\n1.0 \\n\\nVarious \\n\\nIncorporated the overall questions related to the extent of \\nimplementation of policies and procedures within individual \\nmetrics \\nClarified metric questions related to the integration of \\ncybersecurity risk management and enterprise risk management  \\n\\nThroughout \\ndomains \\n\\nQ’s 5, 7, 9, \\nand 10 \\n\\n1.0 \\n\\nVarious \\n\\nAdded a new metric on vulnerability disclosure practices \\n\\nQ 24 \\n\\n1.0 \\n\\n3/12/2021  Draft issued for comment \\n\\n1.1 \\n\\n4/6/2021 – \\n5/12/2021 \\n\\n1.1 \\n\\n5/12/2021 \\n\\nAddressed comments received  \\n\\nIncluded a proposed weighted average maturity calculation for \\nconsideration in a future update to these metrics \\n\\nPg. 10-11 \\n\\nAll \\n\\nVarious \\n\\n1.1 \\n\\n5/12/2021  Added a new Supply Chain Risk Management domain within \\nIdentify, which will not affect the framework function score. \\n\\nPg. 23-26 \\n\\n1.1 \\n\\n5/12/2021 \\n\\nFinal issued \\n\\nAll \\n\\nPage 2 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nContents \\n\\nGENERAL INSTRUCTIONS .................................................................................................................... 4 \\n\\nOverview .................................................................................................................................................. 4 \\n\\nSubmission Deadline ............................................................................................................................. 4 \\n\\nBackground and Methodology.............................................................................................................. 4 \\n\\nFISMA Metrics Ratings .......................................................................................................................... 6 \\n\\nKey Changes to the FY 2021 IG FISMA Metrics ............................................................................... 7 \\n\\nFrequently Asked Questions ................................................................................................................ 9 \\n\\nProposed Weighted Metrics Calculation ........................................................................................... 10 \\n\\nIDENTIFY FUNCTION AREA ................................................................................................................. 13 \\n\\nTable 5: Risk Management ................................................................................................................. 13 \\n\\nTable 6: Supply Chain Risk Management (SCRM) ........................................................................ 23 \\n\\nPROTECT FUNCTION AREA ................................................................................................................ 27 \\n\\nTable 7: Configuration Management ................................................................................................. 27 \\n\\nTable 8: Identity and Access Management ...................................................................................... 33 \\n\\nTable 9: Data Protection and Privacy ................................................................................................ 40 \\n\\nTable 10: Security Training ................................................................................................................. 44 \\n\\nDETECT FUNCTION AREA ................................................................................................................... 48 \\n\\nTable 11: ISCM ..................................................................................................................................... 48 \\n\\nRESPOND FUNCTION AREA ............................................................................................................... 51 \\n\\nTable 12: Incident Response .............................................................................................................. 51 \\n\\nRECOVER FUNCTION AREA ............................................................................................................... 56 \\n\\nTable 13: Contingency Planning ........................................................................................................ 56 \\n\\nPage 3 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nGENERAL INSTRUCTIONS \\n\\nOverview  \\nThe Federal Information Security Modernization Act of 2014 (FISMA) requires each agency Inspector \\nGeneral (IG), or an independent external auditor, to conduct an annual independent evaluation to \\ndetermine the effectiveness of the information security program and practices of its respective agency. \\nAccordingly, the fiscal year (FY) 2021 IG FISMA Reporting Metrics contained in this document provide \\nreporting requirements across key areas to be addressed in the independent evaluations of agencies’ \\ninformation security programs. \\n\\nSubmission Deadline \\nIn accordance with FISMA and Office of Management and Budget (OMB) Memorandum M-21-02, \\nFiscal Year 2020-2021 Guidance on Federal Information Security and Privacy Management \\nRequirements, all Federal agencies are to submit their IG metrics into the Department of Homeland \\nSecurity’s (DHS) CyberScope application by October 29, 2021. IG evaluations should reflect the status of \\nagency information security programs from the completion of testing/fieldwork conducted for FISMA in \\n2021. Furthermore, IGs are encouraged to work with management at their respective agencies to establish \\na cutoff date to facilitate timely and comprehensive evaluation of the effectiveness of information security \\nprograms and controls. \\n\\nBackground and Methodology \\nThe FY 2021 IG FISMA Reporting Metrics were developed as a collaborative effort amongst OMB, \\nDHS, and the Council of the Inspectors General on Integrity and Efficiency (CIGIE), in consultation with \\nthe Federal Chief Information Officer (CIO) Council and other stakeholders. The FY 2021 metrics \\nrepresent a continuation of work begun in FY 2016, when the IG metrics were aligned with the five \\nfunction areas in the National Institute of Standards and Technology (NIST) Framework for Improving \\nCritical Infrastructure Cybersecurity (Cybersecurity Framework): Identify, Protect, Detect, Respond, and \\nRecover. The Cybersecurity Framework provides agencies with a common structure for identifying and \\nmanaging cybersecurity risks across the enterprise and provides IGs with guidance for assessing the \\nmaturity of controls to address those risks. \\n\\nTable 1 below provides an overview of the IG metrics by NIST Cybersecurity Framework (CSF) function \\narea and related categories. The FY 21 IG metrics include a new Supply Chain Risk Management domain \\nwithin the Identify function area.  \\n\\nPage 4 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nTable 1: IG Metrics and NIST Cybersecurity Framework Function Areas and Categories \\n\\nIG Metric Function Area and Related Domains1 \\nIdentify (Risk Management) \\n\\nIdentify (Supply Chain Risk Management) \\nProtect (Configuration Management) \\n\\nProtect (Identity and Access Management) \\nProtect (Data Protection and Privacy) \\nProtect (Security Training) \\nDetect (Information Security Continuous \\nMonitoring) \\nRespond (Incident Response) \\n\\nRecover (Contingency Planning) \\n\\nRelated CSF Categories \\nAsset Management (ID.AM), Business Environment \\n(ID.BE), Governance (ID.GV), Risk Assessment \\n(ID.RA), and Risk Management Strategy (ID.RM) \\nSupply Chain Risk Management (ID.SC) \\nInformation Protection Processes and Procedures \\n(PR.IP) \\nIdentity Management and Access Control (PR.AC) \\nData Security (PR.DS) \\nAwareness and Training (PR.AT) \\n\\nSecurity Continuous Monitoring (DE.CM) \\nResponse Planning (RS.RP), Communications \\n(RS.CO), Analysis (RS.AN), Mitigation (RS.MI), and \\nImprovements (RS.IM) \\nRecovery Planning (RC.RP), Improvements (RC.IM), \\nand Communications (RC.CO) \\n\\nIGs are required to assess the effectiveness of information security programs on a maturity model \\nspectrum, in which the foundational levels ensure that agencies develop sound policies and procedures \\nand the advanced levels capture the extent that agencies institutionalize those policies and procedures. \\nTable 2 below details the five maturity model levels: ad hoc, defined, consistently implemented, managed \\nand measurable, and optimized.2 Within the context of the maturity model, a Level 4, Managed and \\nMeasurable, information security program is operating at an effective level of security. NIST provides \\nadditional guidance for determining effectiveness of security controls.3 IGs should consider both their and \\nmanagement’s assessment of the unique missions, resources, and challenges when assessing the maturity \\nof information security programs. Management’s consideration of agency mission, resources, and \\nchallenges should be documented in the agency’s assessment of risk as discussed in OMB Circular A-\\n123, the U.S. Government Accountability Office’s (GAO) Green Book, and NIST SP 800-37/800-39.  \\n\\n1 Please refer to the NIST glossary available at https://csrc.nist.gov/glossary for definitions of the function areas \\nand domains. \\n2 The maturity level descriptions outlined in Table 2 provide foundational principles that guided the definition of the \\nspecific maturity level indicators and capabilities outlined in the IG metric questions. IGs should consider these \\ndescriptions when concluding on the overall effectiveness of specific functions, domains, and the information \\nsecurity program overall. \\n3 NIST Special Publication (SP) 800-53, Rev. 5, Security and Privacy Controls for Information Systems and \\nOrganizations, defines security control effectiveness as the extent to which the controls are implemented correctly, \\noperating as intended, and producing the desired outcome with respect to meeting the security requirements for the \\ninformation system in its operational environment or enforcing/mediating established security policies. \\n\\nPage 5 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nTable 2: IG Evaluation Maturity Levels \\n\\nMaturity Level \\n\\nMaturity Level Description \\n\\nLevel 1: Ad-hoc \\n\\nPolicies, procedures, and strategies are not formalized; activities are \\nperformed in an ad-hoc, reactive manner. \\n\\nLevel 2: Defined \\n\\nPolicies, procedures, and strategies are formalized and documented but not \\nconsistently implemented. \\n\\nLevel 3: Consistently \\nImplemented \\n\\nPolicies, procedures, and strategies are consistently implemented, but \\nquantitative and qualitative effectiveness measures are lacking. \\n\\nLevel 4: Managed and \\n\\nMeasurable \\n\\nQuantitative and qualitative measures on the effectiveness of policies, \\nprocedures, and strategies are collected across the organization and used to \\nassess them and make necessary changes. \\n\\nLevel 5: Optimized \\n\\nPolicies, procedures, and strategies are fully institutionalized, repeatable, \\nself-generating, consistently implemented, and regularly updated based on a \\nchanging threat and technology landscape and business/mission needs. \\n\\nFISMA Metrics Ratings \\nAs noted earlier, each agency has a unique mission, cybersecurity challenges, and resources to address \\nthose challenges. Within the maturity model context, agencies should perform a risk assessment and \\nidentify the optimal maturity level that achieves cost-effective security based on their missions and risks \\nfaced, risk appetite, and risk tolerance level. The results of this assessment should be considered by IGs \\nwhen determining effectiveness ratings with respect to the FISMA metrics. For example, if an agency has \\ndefined and formalized specific parameters (e.g. control parameters/tailoring decisions documented in \\nsecurity plans/risk assessments), IGs should consider the applicability of these parameters and determine \\nwhether to consider these when making maturity determinations.  \\n\\nRatings throughout the nine domains will be determined by a simple majority, where the most frequent \\nlevel (i.e. mode) across the questions will serve as the domain rating. For example, if there are seven \\nquestions in a domain, and the agency receives Defined ratings for three questions and Managed and \\nMeasurable ratings for four questions, then the domain rating is Managed and Measurable. OMB and \\nDHS will ensure that these domain ratings are automatically scored when entered into CyberScope, and \\nIGs and CIOs should note that these scores will rate the agency at the higher level in instances when two \\nor more levels are the most frequently rated.  \\n\\nSimilar to FY 2020, IGs have the discretion to determine the overall effectiveness rating and the rating for \\neach of the Cybersecurity Framework functions (e.g., Protect, Detect) at the maturity level of their \\nchoosing. For FY 2021, IG’s also have the discretion to determine the overall effectiveness rating at the \\ndomain (e.g., supply chain risk management, configuration management) level. Using this approach, the \\nIG may determine that a particular domain, function area, and/or the agency’s information security \\nprogram is effective at maturity level lower than Level 4. The rationale for this is to provide greater \\nflexibility for the IGs, while considering the agency-specific factors discussed above.  \\n\\nOMB strongly encourages IGs to use the domain ratings to inform the overall function ratings, and to use \\nthe five function ratings to inform the overall agency rating. For example, if the majority of an agency’s \\nratings in the Protect-Configuration Management, Protect-Identity and Access Management, Protect-Data \\n\\nPage 6 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nProtection and Privacy, and Protect-Security Training domains are Managed and Measurable, the IGs are \\nencouraged to rate the agency’s Protect function as Managed and Measurable. Similarly, IGs are \\nencouraged to apply the same simple majority rule described above to inform the overall agency rating.  \\nIGs should provide comments in CyberScope to explain the rationale for their effectiveness ratings. \\nFurthermore, in CyberScope, IGs will be required to provide comments explaining the rationale for why a \\ngiven metric is rated lower than a Level 4 maturity. Comments in CyberScope should reference how the \\nagency’s risk appetite and tolerance level with respect to cost-effective security, including compensating \\ncontrols, were factored into the IGs decision.  \\n\\nKey Changes to the FY 2021 IG FISMA Metrics  \\nOne of the goals of the annual FISMA evaluations is to assess agencies’ progress toward achieving \\noutcomes that strengthen Federal cybersecurity, including implementing the Administration’s priorities \\nand best practices. One such area is increasing the maturity of the Federal government’s Supply Chain \\nRisk Management (SCRM) practices. As noted in the Federal Acquisition Supply Chain Security Act of \\n2018, agencies are required to assess, avoid, mitigate, accept, or transfer supply chain risks. The FY 2021 \\nIG FISMA Reporting Metrics include a new domain on Supply Chain Risk Management (SCRM) within \\nthe Identify function. This new domain focuses on the maturity of agency SCRM strategies, policies and \\nprocedures, plans, and processes to ensure that products, system components, systems, and services of \\nexternal providers are consistent with the organization’s cybersecurity and supply chain risk management \\nrequirements. The new domain references SCRM criteria in NIST Special Publication (SP) 800-53, Rev. \\n5, Security and Privacy Controls for Information Systems and Organizations. To provide agencies with \\nsufficient time to fully implement NIST 800-53, Rev 5., in accordance with OMB A-130, these new \\nmetrics should not be considered for the purposes of the Identify framework function rating. \\n\\nAlso, within the Identify function, specific metric questions have been reorganized and reworded to focus \\non the degree to which cyber risk management processes are integrated with enterprise risk management \\n(ERM) processes. As an example, IGs are directed to evaluate how cybersecurity risk registers are used to \\ncommunicate information at the information system, mission/business process, and organizational levels. \\nThese changes are consistent with NIST Interagency Report 8286, “Integrating Cybersecurity and \\nEnterprise Risk Management (ERM),” which provides guidance to help organizations improve the \\ncybersecurity risk information they provide as inputs to their enterprise ERM programs.4  \\n\\nFurthermore, OMB has issued guidance on improving vulnerability identification, management, and \\nremediation. Specifically, Memorandum M-20-32, Improving Vulnerability Identification, Management, \\nand Remediation, September 2, 2020, provides guidance to federal agencies on collaborating with \\nmembers of the public to find and report vulnerabilities on federal information systems. In addition, DHS \\nBinding Operational Directive 20-01, Develop and Publish a Vulnerability Disclosure Policy, September \\n2, 2020, provides guidance on the development and publishing of an agency’s vulnerability disclosure \\npolicy and supporting handling procedures. The IG FISMA Reporting Metrics include a new question \\n(#24) to measure the extent to which agencies utilize a vulnerability disclosure policy (VDP) as part of \\ntheir vulnerability management program for internet-accessible federal systems. \\n\\nIn addition, the IG metric questions related to the implementation of policies and procedures have been \\nreorganized and streamlined to reduce duplication and redundancies. Furthermore, a new Frequently \\nAsked Question’s (FAQ) section provides additional guidance to IGs. \\n\\n4 NISTIR 8286, Integrating Cybersecurity and Enterprise Risk Management (ERM), October 2020.  \\n\\nPage 7 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nFISMA Metrics Evaluation Guide \\nOne of the goals of the maturity model reporting approach is to ensure consistency in IG FISMA \\nevaluations across the Federal government. To that end in FY 2018, a collaborative effort amongst OMB, \\nDHS, and CIGIE was undertaken to develop an evaluation guide to accompany the IG FISMA metrics. \\nThe guide is designed to provide a baseline of suggested sources of evidence that can be used by IGs as \\npart of their FISMA evaluations. The guide also includes suggested types of analysis that IGs may \\nperform to assess capabilities in given areas. In FY 2019, the evaluation guide was strengthened to \\ninclude more detailed testing steps and methodologies for IGs to utilize in the function area of Identify \\n(Risk Management). While updates to the evaluation guide were not made in FY 2020, OMB, DHS, and \\nCIGIE plan to continue to enhance the evaluation guide to cover all function areas.5  \\n\\n5 Updates to the evaluation guide will be posted on the DHS FISMA website subsequent to issuance of the metrics. \\n\\nPage 8 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nFrequently Asked Questions  \\n\\n1.  To what extent should IGs utilize NIST SP 800-53, Rev. 5, as criteria for FISMA FY 2021 \\n\\nevaluations? \\n\\n• \\n\\nIn accordance with OMB A-130, agencies are expected to meet the requirements of, and be in \\ncompliance with, NIST standards and guidelines within one year of their respective publication \\ndates unless otherwise directed by OMB. The one-year compliance date for revisions to NIST \\npublications applies only to new or updated material in the publications. For information systems \\nunder development or for legacy systems undergoing significant changes, agencies are expected \\nto meet the requirements of, and be in compliance with, NIST standards and guidelines \\nimmediately upon deployment of the systems.  \\n\\n•  The IG FISMA metrics reference NIST SP 800-53, Rev. 4 criteria for all domains, except the \\n\\nSCRM domain. As applicable and in accordance with OMB A-130, IGs should utilize NIST SP \\n800-53, Rev. 5 as criteria for systems under development or legacy systems undergoing \\nsignificant changes. Due to government-wide priorities and focus areas, IGs should utilize NIST \\n800-53, Rev. 5 as criteria for determining agency maturity in the SCRM domain and related \\nmetrics. \\n\\n2.  Do agencies need to meet all (100%) of the maturity indicators and criteria for previous levels before \\n\\nthey can be rated at a higher maturity level?  \\n\\n•  No. FISMA requires agencies to provide information security protections commensurate with the \\nrisk and magnitude of the harm resulting from unauthorized access, use, disclosure, disruption, \\nmodification, or destruction of information and in information systems. As noted earlier, IGs \\nshould consider both their and management’s assessment of the unique missions, resources, and \\nchallenges when assessing the maturity of information security programs. As such, IGs should \\nuse a risk-based approach when determining the impact of control deficiencies on overall \\nmaturity levels. IGs are encouraged not to use total compliance across maturity indicators and IG \\ntest cases, by itself, as a sole determinant of agency maturity. \\n\\n3.  Does a control exception (one or a few) identified in a sample automatically preclude an Agency from \\n\\nreceiving a particular maturity rating? \\n\\n•  No. As noted above, IGs should use a risk-based approach when determining the impact of \\n\\ncontrol deficiencies on overall maturity levels. IGs are encouraged not to use total compliance \\nacross maturity indicators and IG test cases, by itself, as a sole determinant of agency maturity. \\nIG should consider compensating controls and other agency-specific risk factors. \\n\\n4.  With respect to the Identity-Risk Management section, updates have been made to questions on \\n\\ncybersecurity and enterprise risk management. Are IGs being directed to audit or evaluate agencies’ \\nenterprise risk management programs? \\n\\n•  No. The intent of these questions is to gauge the degree of integration between cyber risk \\nmanagement and ERM.  IGs are encouraged to refer to NIST Interagency Report 8286, \\nIntegrating Cybersecurity and Enterprise Risk Management, for additional information.  \\n\\nPage 9 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nProposed Weighted Metrics Calculation \\nSince the FY 2017 FISMA reporting process, IGs have been directed to utilize a mode-based scoring \\napproach to assess agency maturity levels. Under this approach, ratings throughout the reporting domains \\nwere determined by a simple majority, where the most frequent level (i.e., the mode) across the questions \\nserved as the domain rating. The same logic was applied at the function and overall information security \\nprogram level. While this approach has provided an important baseline measure of the maturity of \\nagencies’ information security programs, all the metric questions have been weighted equally. \\n\\nTo drive continued improvements in cybersecurity maturity across the federal landscape and focus agency \\nefforts, this document introduces a pilot concept of weighting specific FISMA metrics for assessment and \\nscoring. Ten priority metrics, shown in Table 3, have been proposed based on a combination of the lowest \\naverage performing metrics from previous assessments, administration priorities, and the highest value \\ncontrols. As part of the proposed weighted average approach to scoring, these priority metrics would be \\nweighted twice as much in the maturity calculation, which is described in greater detail below. This pilot \\napproach will help evaluate the impacts of these metrics and prepare agencies for the possibility of \\nchanging the calculation process in a future update to this document. \\n\\nThe overall maturity of the agency’s information security program would be calculated based on the \\naverage rating of the individual function areas (Identify, Protect, Detect, Respond, and Recover). For \\nexample, if the weighted average maturity of two of the function areas is Level 3 – Consistently \\nImplemented, and Level 4 – Managed and Measurable for the remaining three areas, then the information \\nsecurity program rating (average of 3.60) would be Level 4 – Managed and Measurable. The outcomes of \\nthis pilot will be shared with the CISO council and CIGIE for further consideration. Table 4 below \\nprovides a hypothetical example of an IG evaluation for the Identify-Risk Management area. Priority \\nmetrics within the Identity-Risk Management domain are highlighted in blue.  \\n\\nPage 10 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nTable 3: Proposed Priority Metrics \\n\\nMetric  Description \\n5 \\n\\nCybersecurity risk \\nmanagement and \\nintegration with \\nenterprise risk \\nmanagement (ERM) \\nAutomated view of \\nrisk \\n\\nStrong \\nauthentication \\nmeasures – \\nprivileged users \\nLeast privilege and \\nseparation of duties \\n\\nCybersecurity \\nFunction and \\nDomain \\nIdentify – Risk \\nManagement \\n\\nIdentify – Risk \\nManagement \\n\\nProtect – Identity \\nand Access \\nManagement \\n\\nProtect – Identity \\nand Access \\nManagement \\n\\nPII security controls  Protect – Data \\nProtection and \\nPrivacy \\nProtect – Data \\nProtection and \\nPrivacy \\nDetect – ISCM \\n\\nSecurity controls for \\nexfiltration \\n\\nInformation Security \\nContinuous \\nMonitoring (ISCM) \\npolicies and strategy \\nIncident detection \\nand analysis \\n\\nRespond – \\nIncident Response \\n\\n10 \\n\\n31 \\n\\n32 \\n\\n36 \\n\\n37 \\n\\n47 \\n\\n54 \\n\\n55 \\n\\nIncident handling \\n\\nRespond – \\nIncident Response \\n\\n63 \\n\\nTesting of \\ninformation system \\ncontingency plans \\n\\nRecover – \\nContingency \\nPlanning \\n\\nReason \\nSupports Administration’s focus to improve \\nintegration of cybersecurity risk management within \\nbroader organizational risk management to help \\ndrive conversations for additional cybersecurity \\nresources.  \\nImproves government’s ability to report and analyze \\ncybersecurity data for use in decision making, \\nsupports Administration’s focus on automated \\nreporting. \\nSupports Administration’s focus on zero trust \\narchitectures, reducing privilege escalation, and \\nimplementation of M-19-17. \\n\\nSupports Administration’s focus on zero trust \\narchitectures, reducing privilege escalation, and \\nimplementation of M-19-17. \\nSupports Administration’s focus on encrypting data \\nat rest and in transit. \\n\\nSupports Administration’s focus on encrypting data \\nat rest and in transit. \\n\\nImproves government’s ability to report and analyze \\ncybersecurity data for use in decision making, \\nsupports Administration’s focus on automated \\nreporting. \\nSupports Administration's focus on continued \\nimprovement of incident detection and handling to \\nboth address Congressional inquiries, as well as \\nimproving the government's ability to better identify \\nand respond to the continued advancement of tactics \\nused by adversaries around the world. \\nSupports Administration's focus on continued \\nimprovement of incident detection and handling to \\nboth address Congressional inquiries, as well as \\nimproving the government's ability to better identify \\nand respond to the continued advancement of tactics \\nused by adversaries around the world. \\nCritical component to Continuity of Operations Plans \\n(COOPs), where rapid shift to telework from \\nCOVID-19 and SolarWinds incidents are recent \\nexamples of the importance of testing these plans.  \\n\\nPage 11 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\n\\nTable 4: Example of Proposed Weighted Average Maturity Calculation  \\n\\nMetric \\nNumber \\n\\nMetric Descriptor \\n\\nIG Rating (Weight)  \\n\\nWeighted Factor \\n\\n1 \\n\\n2 \\n\\n3 \\n\\n4 \\n\\n5 \\n\\n6 \\n\\n7 \\n\\n8 \\n\\n9 \\n\\nInventory \\n\\nHardware asset \\nmanagement \\n\\nSoftware asset \\nmanagement \\n\\nLevel 4 (1) \\n\\nLevel 3 (1) \\n\\nLevel 2 (1) \\n\\nSystem categorization \\n\\nLevel 4 (1) \\n\\nCybersecurity risk \\nmanagement and \\nintegration with ERM \\n\\nInformation security \\narchitecture \\n\\nLevel 2 (2) \\n\\nLevel 4 (1) \\n\\nRoles and responsibilities  Level 3 (1) \\n\\nPOA&M \\n\\nLevel 3 (1) \\n\\nRisk communication \\n\\nLevel 4 (1) \\n\\n10 \\n\\nAutomated View of Risk  Level 3 (2)  \\n\\n4 \\n\\n3 \\n\\n2 \\n\\n4 \\n\\n4 \\n\\n4 \\n\\n3 \\n\\n3 \\n\\n4 \\n\\n6 \\n\\nTotal \\n\\n12* \\n\\n37 \\n\\n* The Weighted Average is calculated by multiplying selected metrics by the Priority Metric Weight of \\n2.0 and then dividing the new total for each domain. For example, the Risk Management domain has 10 \\nmetrics of which 2 are Priority metrics so the total maturity for this domain is then divided by 12 \\ninstead of 10.) \\n\\nWeighted Average Maturity = 3.08 = Level 3 Consistently Implemented6 \\n\\nThis same approach would be used for all domains and function areas. The rating for the Protect function \\nis a weighted average of all metrics in the domains that comprise the Protect function, such as the Protect-\\nConfiguration Management, Protect-Identity and Access, Protect-Security Training, and Protect-Data \\nProtection and Privacy domains. The overall information security program maturity rating is then an \\naverage of the function level ratings. \\n\\n6 Weighted average maturities will be calculated to two decimal points and rounded to the nearest whole number \\n(i.e., if the number after the decimal point is less than 5, it will be rounded down to the next lower maturity level; if \\nthe  number is greater than or equal to 5, it will be rounded up to the next higher maturity level). \\n\\nPage 12 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nIDENTIFY FUNCTION AREA  \\nTable 5: Risk Management \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes for \\ndeveloping and maintaining a \\ncomprehensive and accurate \\ninventory of its information \\nsystems and system \\ninterconnections. \\n\\nThe organization has defined \\nits policies, procedures, and \\nprocesses for developing and \\nmaintaining a comprehensive \\nand accurate inventory of its \\ninformation systems and \\nsystem interconnections. \\n\\nThe organization maintains a \\ncomprehensive and accurate \\ninventory of its information \\nsystems (including cloud \\nsystems, public-facing \\nwebsites, and third-party \\nsystems), and system \\ninterconnections. \\n\\nThe organization ensures that \\nthe information systems \\nincluded in its inventory are \\nsubject to the monitoring \\nprocesses defined within the \\norganization's ISCM strategy. \\n\\nThe organization uses \\nautomation to develop and \\nmaintain a centralized \\ninformation system inventory \\nthat includes hardware and \\nsoftware components from all \\norganizational information \\nsystems. The centralized \\ninventory is updated in a near-\\nreal time basis. \\n\\nQuestion \\n\\n1. \\n\\nTo what extent does the \\norganization maintain a \\ncomprehensive and accurate \\ninventory of its information \\nsystems (including cloud systems, \\npublic facing websites, and third-\\nparty systems), and system \\ninterconnections (NIST SP 800-\\n53. Rev. 4: CA-3, PM-5, and CM-\\n8; NIST Cybersecurity \\nFramework (CSF): ID.AM-1 – 4; \\nFY 2021 CIO FISMA Metrics: \\n1.1, 1.1.5 and 1.4, OMB A-130, \\nNIST SP 800-37, Rev. 2: Task P-\\n18).  \\n\\nPage 13 of 60 \\n\\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\n2. \\n\\nQuestion \\n\\nTo what extent does the \\norganization use standard data \\nelements/taxonomy to develop \\nand maintain an up-to-date \\ninventory of hardware assets \\n(including GFE and Bring \\nYour Own Device (BYOD) \\nmobile devices) connected to \\nthe organization’s network \\nwith the detailed information \\nnecessary for tracking and \\nreporting (NIST SP 800-53 \\nRev. 4: CA-7 and CM-8; NIST \\nSP 800-137; NIST IR 8011; \\nFederal Enterprise \\nArchitecture (FEA) \\nFramework, v2; FY 2021 CIO \\nFISMA Metrics: 1.2, 1.3, 2.2, \\n3.9,   CSF: ID.AM-1; NIST SP \\n800-37, Rev. 2: Task P-10). \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined policies, procedures, \\nand processes for using \\nstandard data \\nelements/taxonomy to develop \\nand maintain an up-to-date \\ninventory of hardware assets \\nconnected to the organization’s \\nnetwork with the detailed \\ninformation necessary for \\ntracking and reporting. \\n\\nThe organization has defined \\npolicies, procedures, and \\nprocesses for using standard \\ndata elements/taxonomy to \\ndevelop and maintain an up-\\nto-date inventory of hardware \\nassets connected to the \\norganization’s network with \\nthe detailed information \\nnecessary for tracking and \\nreporting. \\n\\nThe organization consistently \\nutilizes its standard data \\nelements/taxonomy to develop \\nand maintain an up-to-date \\ninventory of hardware assets \\nconnected to the organization’s \\nnetwork and uses this \\ntaxonomy to inform which \\nassets can/cannot be \\nintroduced into the network. \\n\\nThe organization ensures that \\nthe hardware assets connected \\nto the network are covered by \\nan organization-wide \\nhardware asset management \\ncapability and are subject to \\nthe monitoring processes \\ndefined within the \\norganization's ISCM strategy. \\n\\nFor mobile devices, the \\nagency enforces the capability \\nto deny access to agency \\nenterprise services when \\nsecurity and operating system \\nupdates have not been applied \\nwithin a given period based on \\nagency policy or guidance. \\n\\nThe organization employs \\nautomation to track the life \\ncycle of the organization's \\nhardware assets with processes \\nthat limit the \\nmanual/procedural methods for \\nasset management. Further, \\nhardware inventories are \\nregularly updated as part of the \\norganization’s enterprise \\narchitecture current and future \\nstates. \\n\\nPage 14 of 60 \\n\\n \\n\\x0c3. \\n\\nQuestion \\n\\nTo what extent does the \\norganization use standard data \\nelements/taxonomy to develop \\nand maintain an up-to-date \\ninventory of the software and \\nassociated licenses used within \\nthe organization with the \\ndetailed information necessary \\nfor tracking and reporting \\n(NIST SP 800-53 Rev. 4: CA-\\n7, CM-8, and CM-10; NIST \\nSP 800-137; NIST IR 8011; \\nFEA Framework, v2; FY 2021 \\nCIO FISMA Metrics: 1.2.5, \\n1.3.3, 1.3.9, 1.3.10, 3.10; CSF: \\nID.AM-2; NIST SP 800-37, \\nRev. 2: Task P-10)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined policies, procedures, \\nand processes for using \\nstandard data \\nelements/taxonomy to develop \\nand maintain an up-to-date \\ninventory of software assets \\nand licenses, including for \\nmobile applications, utilized in \\nthe organization's environment \\nwith the detailed information \\nnecessary for tracking and \\nreporting. \\n\\nThe organization has defined \\npolicies, procedures, and \\nprocesses for using standard \\ndata elements/taxonomy to \\ndevelop and maintain an up-to-\\ndate inventory of software \\nassets and licenses, including \\nfor mobile applications, \\nutilized in the organization's \\nenvironment with the detailed \\ninformation necessary for \\ntracking and reporting. \\n\\nThe organization consistently \\nutilizes its standard data \\nelements/taxonomy to \\ndevelop and maintain an up-\\nto-date inventory of software \\nassets and licenses, including \\nfor mobile applications,  \\nutilized in the organization's \\nenvironment and uses this \\ntaxonomy to inform which \\nassets can/cannot be \\nintroduced into the network. \\n\\nThe organization employs \\nautomation to track the life \\ncycle of the organization's \\nsoftware assets (and their \\nassociated licenses), including \\nfor mobile applications, with \\nprocesses that limit the \\nmanual/procedural methods for \\nasset management. Further, \\nsoftware inventories are \\nregularly updated as part of the \\norganization’s enterprise \\narchitecture current and future \\nstates. \\n\\nThe organization ensures that \\nthe software assets, including \\nmobile applications as \\nappropriate, on the network \\n(and their associated \\nlicenses), are covered by an \\norganization-wide software \\nasset management (or Mobile \\nDevice Management) \\ncapability and are subject to \\nthe monitoring processes \\ndefined within the \\norganization's ISCM strategy. \\n\\nFor mobile devices, the \\nagency enforces the capability \\nto prevent the execution of \\nunauthorized software (e.g., \\nblacklist, whitelist, or \\ncryptographic \\ncontainerization).  \\n\\nPage 15 of 60 \\n\\n \\n\\x0c4. \\n\\nQuestion \\n\\nTo what extent has the \\norganization categorized and \\ncommunicated the \\nimportance/priority of \\ninformation systems in enabling \\nits missions and business \\nfunctions, including for high \\nvalue assets (NIST SP 800-53 \\nRev. 4: RA-2, PM-7, and PM-\\n11; NIST SP 800-60; NIST SP \\n800-37 (Rev. 2); CSF: ID.BE-3, \\nID.AM-5, and ID.SC-2; FIPS \\n199; FY 2021 CIO FISMA \\nMetrics: 1.1; OMB M-19-03; \\nNIST SP 800-37, Rev. 2: Task \\nC-2, C-3, P-4, P-12, P-13, S-1 – \\nS-3, NIST IR 8170 )? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization ensures the \\nrisk-based allocation of \\nresources based on system \\ncategorization, including for the \\nprotection of high value assets, \\nas appropriate, through \\ncollaboration and data-driven \\nprioritization.  \\n\\nThe organization utilizes \\nimpact-level prioritization for \\nadditional granularity, and \\ncybersecurity framework \\nprofiles, as appropriate, to \\nsupport risk-based decision-\\nmaking. \\n\\nThe organization has not \\ndefined policies, procedures, \\nand processes for categorizing, \\nreviewing, and communicating \\nthe importance/priority of \\ninformation systems in \\nenabling its missions and \\nbusiness functions, including \\nfor high value assets, as \\nappropriate. \\n\\nThe organization has defined \\npolicies, procedures, and \\nprocesses for categorizing, \\nreviewing, and \\ncommunicating the \\nimportance/priority of \\ninformation systems in \\nenabling its missions and \\nbusiness functions, including \\nfor high value assets, as \\nappropriate. \\n\\nIn addition, the organization \\nhas not defined its policies, \\nprocedures, and processes for \\ncontrols allocation, selection, \\nand tailoring based on the \\nimportance/priority of its \\ninformation systems. \\n\\nIn addition, the organization \\nhas defined policies, \\nprocedures, and processes for \\ncontrols allocation, selection \\nand tailoring based on the \\nimportance/priority of its \\ninformation systems. \\n\\nThe organization \\nconsistently implements its \\npolicies, procedures, and \\nprocesses for system \\ncategorization, review, and \\ncommunication, including \\nfor high value assets, as \\nappropriate. Security \\ncategorizations consider \\npotential adverse impacts \\nto organization operations, \\norganizational assets, \\nindividuals, other \\norganizations, and the \\nNation.  System \\ncategorization levels are \\nused to guide risk \\nmanagement decisions, \\nsuch as the allocation, \\nselection, and \\nimplementation of \\nappropriate control \\nbaselines.  \\n\\nPage 16 of 60 \\n\\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\n5. \\n\\nTo what extent does the \\norganization ensure that \\ninformation system security risks \\nare adequately managed at  the \\norganizational, mission/business \\nprocess, and information system \\nlevels (NIST SP 800-39; NIST \\nSP 800-53 Rev. 4: RA-3, PM-9; \\nNIST IR 8286, CSF: ID RM-1 – \\nID.RM-3; OMB A-123; OMB \\nM-16-17; OMB M-17-25; NIST \\nSP 800-37 (Rev. 2):  Tasks P-2, \\nP-3, P-14, R-2, and R-3? \\n\\nThe organization has not \\ndefined and communicated \\nthe policies, procedures and \\nprocesses it utilizes to \\nmanage the cybersecurity \\nrisks associated with \\noperating and maintaining \\nits information systems. At a \\nminimum, the policies, \\nprocedures, and processes \\ndo not cover the following \\nareas from a cybersecurity \\nperspective: \\n\\n- \\n- \\n- \\n- \\n\\nRisk Framing \\nRisk assessment \\nRisk response \\nRisk monitoring \\n\\nThe organization has \\ndefined and communicated \\nthe policies, procedures and \\nprocesses it utilizes to \\nmanage the cybersecurity \\nrisks associated with \\noperating and maintaining \\nits information systems. The \\npolicies, procedures, and \\nprocesses cover \\ncybersecurity risk \\nmanagement at the \\norganizational, \\nmission/business process, \\nand information system \\nlevels and address the \\nfollowing components \\n\\n- \\n- \\n- \\n- \\n\\nRisk Framing \\nRisk assessment \\nRisk response \\nRisk monitoring  \\n\\nThe organization consistently \\nimplements its policies, \\nprocedures, and processes to \\nmanage the cybersecurity \\nrisks associated with operating \\nand maintaining its \\ninformation systems. The \\norganization ensures that \\ndecisions to manage \\ncybersecurity risk at the \\ninformation system level are \\ninformed and guided by risk \\ndecisions made at the \\norganizational and \\nmission/business levels.  \\n\\nSystem risk assessments are \\nperformed [according to \\norganizational defined time \\nframes] and appropriate \\nsecurity controls to mitigate \\nrisks identified are \\nimplemented on a consistent \\nbasis. The organization \\nutilizes the common \\nvulnerability scoring system, \\nor similar approach, to \\ncommunicate the \\ncharacteristics and severity of \\nsoftware vulnerabilities.  \\n\\nThe organization utilizes the \\nresults of its system level risk \\nassessments, along with other \\ninputs, to perform and \\nmaintain an organization-wide \\ncybersecurity and privacy risk \\nassessment. The result of this \\nassessment is documented in a \\ncybersecurity risk register and \\nserve as an input into the \\norganization’s enterprise risk \\nmanagement program.  The \\norganization consistently \\nmonitors the effectiveness of \\nrisk responses to ensure that \\nrisk tolerances are maintained \\nat an appropriate level. \\n\\nThe organization ensures that \\ninformation in cybersecurity \\nrisk registers is obtained \\naccurately, consistently, and in \\na reproducible format and is \\nused to (i) quantify and \\naggregate security risks, (ii) \\nnormalize cybersecurity risk \\ninformation across \\norganizational units, and (iii) \\nprioritize operational risk \\nresponse \\n\\nThe cybersecurity risk \\nmanagement program is fully \\nintegrated at the \\norganizational, \\nmission/business process, and \\ninformation system levels, as \\nwell as with the entity’s \\nenterprise risk management \\nprogram.  \\n\\nFurther, the organization's \\ncybersecurity risk \\nmanagement program is \\nembedded into daily decision \\nmaking across the \\norganization and provides for \\ncontinuous identification and \\nmonitoring to ensure that risk \\nremains within \\norganizationally-defined \\nacceptable levels.  \\n\\nThe organization utilizes \\nCybersecurity Framework \\nprofiles to align cybersecurity \\noutcomes with mission or \\nbusiness requirements, risk \\ntolerance, and resources of \\nthe organization. \\n\\nFurther, the organization \\nutilizes a cybersecurity risk \\nregister to manage risks, as \\nappropriate, and is consistently \\ncapturing and sharing lessons \\nlearned on the effectiveness of \\ncybersecurity risk management \\nprocesses and updating the \\nprogram accordingly. \\n\\nPage 17 of 60 \\n\\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\n6. \\n\\nQuestion \\n\\nTo what extent does the \\norganization utilize an \\ninformation security \\narchitecture to provide a \\ndisciplined and structured \\nmethodology for managing \\nrisk, including risk from the \\norganization’s supply chain \\n(Federal Information \\nTechnology Acquisition \\nReform Act (FITARA), NIST \\nSP 800-39; NIST SP 800-160; \\nNIST SP 800-37 (Rev. 2) Task \\nP-16; OMB M-19-03; OMB M-\\n15-14, FEA Framework; NIST \\nSP 800-53 Rev. 4: PL-8, SA-3, \\nSA-8, SA-9, SA-12, and PM-9; \\nNIST SP 800-163, Rev. 1 CSF: \\nID.SC-1 and PR.IP-2; SECURE \\nTechnology Act: s. 1326)? \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined an information \\nsecurity architecture and its \\nprocesses for ensuring that \\nnew/acquired \\nhardware/software, \\nincluding mobile apps, are \\nconsistent with its security \\narchitecture prior to \\nintroducing systems into its \\ndevelopment environment.  \\n\\nThe organization has \\ndefined an information \\nsecurity architecture and \\ndescribed how that \\narchitecture is integrated \\ninto and supports the \\norganization’s enterprise \\narchitecture. In addition, the \\norganization has defined \\nhow it implements system \\nsecurity engineering \\nprinciples and software \\nassurance processes for \\nmobile applications, within \\nits system development life \\ncycle (SDLC).  \\n\\nThe organization has \\nconsistently implemented its \\nsecurity architecture across the \\nenterprise, business process, \\nand system levels. System \\nsecurity engineering principles \\nare followed and include \\nassessing the impacts to the \\norganizations information \\nsecurity architecture prior to \\nintroducing information \\nsystem changes into the \\norganization’s environment. \\n\\nThe organization’s \\ninformation security \\narchitecture is integrated with \\nits systems development \\nlifecycle and defines and \\ndirects implementation of \\nsecurity methods, \\nmechanisms, and capabilities \\nto both the Information and \\nCommunications Technology \\n(ICT) supply chain and the \\norganization’s information \\nsystems. \\n\\nThe organization uses \\nadvanced technologies and \\ntechniques for managing \\nsupply chain risks. To the \\nextent practicable, the \\norganization can quickly adapt \\nits information security and \\nenterprise architectures to \\nmitigate supply chain risks.  \\n\\nIn addition, the organization \\nemploys a software assurance \\nprocess for mobile \\napplications. \\n\\nPage 18 of 60 \\n\\n \\n\\x0c7. \\n\\nQuestion \\n\\nTo what extent have the roles \\nand responsibilities of internal \\nand external stakeholders \\ninvolved in cybersecurity risk \\nmanagement processes been \\ndefined, communicated, and \\nimplemented across the \\norganization (NIST SP 800-39: \\nSection 2.3.1, 2.3.2, and \\nAppendix D; NIST SP 800-53 \\nRev. 4: RA-1; CSF: ID.AM-6, \\nID.RM-1, and ID.GV-2; NIST \\nIR 8286, Section 3.1.1, OMB \\nA-123; NIST SP 800-37 (Rev. \\n2) Section 2.8 and Task P-1; \\nOMB M-19-03)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nIndividuals are consistently \\nperforming the cybersecurity \\nrisk management roles and \\nresponsibilities that have \\nbeen defined across the \\norganization. This includes \\nroles and responsibilities \\nrelated to integration with \\nenterprise risk management \\nprocesses, as appropriate. \\n\\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to effectively \\nimplement cybersecurity risk \\nmanagement activities and \\nintegrate those activities with \\nenterprise risk management \\nprocesses, as appropriate. \\nFurther, stakeholders involved \\nin cybersecurity risk \\nmanagement are held \\naccountable for carrying out \\ntheir roles and responsibilities \\neffectively. \\n\\nThe organization utilizes an \\nintegrated governance structure, \\nin accordance with A-123, and \\nassociated review processes \\n(e.g., ERM councils or IT \\ninvestment review boards) to \\nsupport the integration of roles \\nand responsibilities for \\ncybersecurity risk management \\nand ERM. \\n\\nRoles and responsibilities \\nfor cybersecurity risk \\nmanagement have not been \\ndefined and communicated \\nacross the organization.  \\n\\nFurther, the organization has \\nnot defined the relevant \\nwork roles for stages in the \\ncybersecurity risk \\nmanagement process and \\nwhich roles are responsible, \\naccountable, consulted, or \\ninformed about various \\nactivities, as appropriate. In \\naddition, the organization \\nhas not defined the \\nrelationships between \\ncybersecurity risk \\nmanagement roles and those \\nroles involved with \\nenterprise risk management. \\n\\nRoles and responsibilities of \\nstakeholders involved in \\ncybersecurity risk \\nmanagement processes have \\nbeen defined and \\ncommunicated across the \\norganization. This includes \\nthe relevant work roles for \\nstages in the cybersecurity \\nrisk management process \\nand which roles are \\nresponsible, accountable, \\nconsulted, or informed \\nabout various activities, as \\nappropriate. \\n\\nIn addition, the organization \\nhas defined and clearly \\ncommunicated the \\nrelationships between \\ncybersecurity risk \\nmanagement roles and those \\nroles involved with \\nenterprise risk management. \\n\\nPage 19 of 60 \\n\\n \\n\\x0cQuestion \\n\\n8. \\n\\nTo what extent has the \\norganization ensured that plans \\nof action and milestones \\n(POA&Ms) are utilized for \\neffectively mitigating security \\nweaknesses (NIST SP 800-53 \\nRev. 4: CA-5; NIST SP 800-37 \\n(Rev. 2) Task A-6, R-3; OMB \\nM-04-14, M-19-03, CSF v1.1, \\nID.RA-6)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization monitors and \\nanalyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its POA&M activities and \\nuses that information to make \\nappropriate adjustments, as \\nneeded, to ensure that its risk \\nposture is maintained. \\n\\nThe organization employs \\nautomation to correlate \\nsecurity weaknesses amongst \\ninformation systems and \\nidentify enterprise-wide trends \\nand solutions in a near real- \\ntime basis. Furthermore, \\nprocesses are in place to \\nidentify and manage emerging \\nrisks, in addition to known \\nsecurity weaknesses. \\n\\nPolicies and procedures for \\nthe effective use of \\nPOA&Ms to mitigate \\nsecurity weaknesses have \\nnot been defined and \\ncommunicated. \\n\\nPolicies and procedures for \\nthe effective use of \\nPOA&Ms have been \\ndefined and communicated. \\nThese policies and \\nprocedures address, at a \\nminimum, the centralized \\ntracking of security \\nweaknesses, prioritization \\nof remediation efforts, \\nmaintenance, and \\nindependent validation of \\nPOA&M activities. \\n\\nThe organization consistently \\nutilizes POA&Ms to \\neffectively mitigate security \\nweaknesses. The organization \\nutilizes a prioritized and \\nconsistent approach to \\nPOA&Ms that considers:  \\n\\n• \\n• \\n\\n• \\n\\n• \\n\\nSecurity categorizations \\nSpecific control \\ndeficiencies and their \\ncriticality \\nRationale for accepting \\ncertain deficiencies in \\ncontrols \\nPOA&M attributes, in \\naccordance with OMB \\nM-04-14 (e.g., severity \\nand brief description of \\nthe weakness and \\nestimated funding \\nresources required to \\nresolve the weakness) \\n\\nPage 20 of 60 \\n\\n \\n\\x0cQuestion \\n\\n9. \\n\\nTo what extent does the \\norganization ensure that \\ninformation about cybersecurity \\nrisks is communicated in a \\ntimely and effective manner to \\nappropriate internal and \\nexternal stakeholders (OMB A-\\n123; OMB Circular A-11 and \\nOMB M-19-03; CSF: Section \\n3.3; NIST SP 800-37 (Rev. 2) \\nTask M-5; SECURE \\nTechnology Act: s. 1326, NIST \\nIR 8170 and 8286)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined how cybersecurity   \\nrisk information is \\ncommunicated in a timely \\nand effective manner to \\nappropriate internal and \\nexternal stakeholders.  \\n\\nThe organization has \\ndefined how cybersecurity \\nrisks are communicated in a \\ntimely and effective manner \\nto appropriate internal and \\nexternal stakeholders. This \\nincludes the organizations \\npolicies, procedures, and \\nprocesses for utilizing \\ncybersecurity risk registers, \\nor other comparable \\nmechanisms, to share and \\ncoordinate cybersecurity \\nrisk activities. \\n\\nThe organization consistently \\nutilizes a cybersecurity risk \\nregister, or other comparable \\nmechanism to ensure that \\ninformation about risks are \\ncommunicated in a timely and \\neffective manner to \\nappropriate internal and \\nexternal stakeholders with a \\nneed-to-know. Furthermore, \\nthe organization actively \\nshares information with \\npartners to ensure that \\naccurate, current information \\nis being distributed and \\nconsumed.  \\n\\nThe organization employs \\nrobust diagnostic and reporting \\nframeworks, including \\ndashboards that facilitate a \\nportfolio view of cybersecurity \\nrisks across the organization. \\nThe dashboard presents \\nqualitative and quantitative \\nmetrics that provide indicators \\nof cybersecurity risk. \\nCybersecurity risks are \\nintegrated into enterprise level \\ndashboards and reporting \\nframeworks. \\n\\nUsing risk profiles and \\ndynamic reporting \\nmechanisms, cybersecurity \\nrisk information is \\nincorporated into the \\norganization’s enterprise risk \\nmanagement program and \\nutilized to provide a fully \\nintegrated, prioritized, \\nenterprise-wide view of \\norganizational risks to drive \\nstrategic and business \\ndecisions.  \\n\\nCyber risks are normalized and \\ntranslated at the organizational \\nlevel to support a fully \\nintegrated, prioritized, \\nenterprise-wide view of \\norganizational risks to drive \\nstrategic and business \\ndecisions. \\n\\nTo facilitate timely, consistent, \\nand effective communication \\nof cybersecurity risks, the \\norganization ensures that data \\nsupporting the cybersecurity \\nrisk register, or other \\ncomparable mechanism, are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format and is \\nused to \\n\\n-  Quantify and \\n\\naggregate security \\nrisks  \\n-  Normalize \\n\\n- \\n\\ninformation across \\norganizational units \\nPrioritize operational \\nrisk response \\nactivities \\n\\nPage 21 of 60 \\n\\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\nidentified and defined its \\nrequirements for an \\nautomated solution to \\nprovide a centralized, \\nenterprise wide (portfolio) \\nview of cybersecurity risks \\nacross the organization, \\nincluding risk control and \\nremediation activities, \\ndependences, risk \\nscores/levels, and \\nmanagement dashboards. \\n\\nThe organization has identified \\nand defined its requirements \\nfor an automated solution that \\nprovides a centralized, \\nenterprise wide view of \\ncybersecurity risks across the \\norganization, including risk \\ncontrol and remediation \\nactivities, dependencies, risk \\nscores/levels, and management \\ndashboards. \\n\\nThe organization consistently \\nimplements an automated \\nsolution across the enterprise \\nthat provides a centralized, \\nenterprise wide view of \\ncybersecurity risks, including \\nrisk control and remediation \\nactivities, dependencies, risk \\nscores/levels, and \\nmanagement dashboards. All \\nnecessary sources of \\ncybersecurity risk information \\nare integrated into the \\nsolution. \\n\\nThe organization has \\ninstitutionalized the use of \\nadvanced technologies for \\nanalysis of trends and \\nperformance against \\nbenchmarks to continuously \\nimprove its cybersecurity risk \\nmanagement program. \\n\\nThe organization uses \\nautomation to perform \\nscenario analysis and model \\npotential responses, including \\nmodeling the potential impact \\nof a threat exploiting a \\nvulnerability and the resulting \\nimpact to organizational \\nsystems and data. \\n\\nIn addition, the organization \\nensures that cybersecurity risk \\nmanagement information is \\nintegrated into ERM reporting \\ntools, such as a governance, \\nrisk management, and \\ncompliance tool), as \\nappropriate \\n\\nQuestion \\n\\n10.  To what extent does the \\n\\norganization utilize technology/ \\nautomation to provide a \\ncentralized, enterprise wide \\n(portfolio) view of \\ncybersecurity risk management \\nactivities across the \\norganization, including risk \\ncontrol and remediation \\nactivities, dependencies, risk \\nscores/levels, and management \\ndashboards (NIST SP 800-39; \\nOMB A-123 and NIST IR \\n8286)? \\n\\n11.  Provide any additional \\ninformation on the \\neffectiveness (positive or \\nnegative) of the organization’s \\nrisk management program that \\nwas not noted in the questions \\nabove. Taking into \\nconsideration the overall \\nmaturity level generated from \\nthe questions above and based \\non all testing performed, is the \\nrisk management program \\neffective? \\n\\nPage 22 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Supply Chain Risk Management) \\n\\nTable 6: Supply Chain Risk Management (SCRM) \\nNote: This section not to be considered in the Identity framework function rating \\n\\nQuestion \\n\\n12.  To what extent does the \\n\\norganization utilize an \\norganization wide SCRM \\nstrategy to manage the supply \\nchain risks associated with the \\ndevelopment, acquisition, \\nmaintenance, and disposal of \\nsystems, system components, \\nand system services? (The \\nFederal Acquisition Supply \\nChain Security Act of 2018 \\n(H.R. 7327, 41 USC Chap. 13 \\nSub chap. III and Chap. 47, \\nP.L. 115-390) (Dec. 21, 2018), \\nNIST SP 800-53, Rev. 5, PM-\\n30, NIST IR 8276)? \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined and communicated an \\norganization wide SCRM \\nstrategy.  \\n\\nThe organization has defined \\nand communicated an \\norganization wide SCRM \\nstrategy. The strategy \\naddresses: \\n\\n- \\n\\n- \\n\\n- \\n\\nSCRM risk appetite \\nand tolerance \\nSCRM strategies or \\ncontrols \\nProcesses for \\nconsistently \\nevaluating and \\nmonitoring supply \\nchain risk \\n-  Approaches for \\n\\nimplementing and \\ncommunicating the \\nSCRM strategy \\n-  Associated roles and \\nresponsibilities \\n\\nThe organization consistently \\nimplements its SCRM strategy \\nacross the organization and \\nutilizes the strategy to guide \\nsupply chain analyses, \\ncommunication with internal \\nand external partners and \\nstakeholders, and in building \\nconsensus regarding the \\nappropriate resources for \\nSCRM. \\n\\nThe organization monitors \\nand analyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its SCRM strategy and \\nmakes updates, as \\nappropriate. The organization \\nensures that data supporting \\nmetrics are obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nThe organization's SCRM \\nstrategy is fully integrated \\nwith its enterprise risk \\nmanagement strategy and \\nprogram. \\n\\nOn a near real-time basis, the \\norganization actively adapts its \\nSCRM strategy to respond to \\nevolving and sophisticated \\nthreats.  \\n\\nFurther, the organization \\nutilizes lessons learned in \\nimplementation to review and \\nupdate its SCRM strategy in \\nan organization defined \\ntimeframe. \\n\\nPage 23 of 60 \\n\\n \\n\\x0cQuestion \\n\\n13.  To what extent does the \\n\\norganization utilize SCRM \\npolicies and procedures to \\nmanage SCRM activities at all \\norganizational tiers (The \\nFederal Acquisition Supply \\nChain Security Act of 2018, \\nNIST 800-53, Rev. 5, SR-1, \\nNIST CSF v1.1, ID.SC-1 and \\nID.SC-5, NIST IR 8276)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Supply Chain Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined and communicated its \\nSCRM policies, procedures, \\nand processes. \\n\\nThe organization has defined \\nand communicated its SCRM \\npolicies, procedures, and \\nprocesses. As appropriate, the \\npolicies and procedures are \\nguided by the organization \\nwide SCRM strategy (metric \\n#12). \\n\\nThe organization consistently \\nimplements its policies, \\nprocedures, and processes for \\nmanaging supply chain risks \\nfor [organizationally-defined] \\nproducts, systems, and \\nservices provided by third \\nparties.  \\n\\nFurther, the organization \\nutilizes lessons learned in \\nimplementation to review and \\nupdate its SCRM policies, \\nprocedures, and processes in \\nan organization defined \\ntimeframe. \\n\\nAt a minimum, the following \\nareas are addressed \\n- \\n\\nProcedures to facilitate the \\nimplementation of the \\npolicy and the associated \\nbaseline supply chain risk \\nmanagement controls as \\nwell as baseline supply \\nchain related controls in \\nother families. \\nPurpose, scope, SCRM \\nroles and responsibilities, \\nmanagement commitment, \\nand coordination amongst \\norganization entities. \\n\\n- \\n\\nIn a near real-time basis, the \\norganization can update its \\nSCRM policies, procedures, \\nand processes, as appropriate, \\nto respond to evolving and \\nsophisticated threats.  \\n\\nThe organization monitors, \\nanalyzes, and reports on the \\nqualitative and quantitative \\nperformance measures used to \\ngauge the effectiveness of its \\nSCRM policies and procedures \\nand ensures that data \\nsupporting the metrics is \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nThe organization has integrated \\nSCRM processes across its \\nenterprise, including personnel \\nsecurity and physical security \\nprograms, hardware, software, \\nand firmware development \\nprocesses, configuration \\nmanagement tools, techniques, \\nand measures to maintain \\nprovenance (as appropriate); \\nshipping and handling \\nprocedures; and programs, \\nprocesses, or procedures \\nassociated with the production \\nand distribution of supply chain \\nelements. \\n\\nPage 24 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cQuestion \\n\\n14.  To what extent does the \\norganization ensure that \\nproducts, system components, \\nsystems, and services of \\nexternal providers are \\nconsistent with the \\norganization’s cybersecurity \\nand supply chain requirements. \\n(The Federal Acquisition \\nSupply Chain Security Act of \\n2018, NIST SP 800-53 REV. 5: \\nSA-4, SR-3, SR-5, SR-6 (as \\nappropriate); NIST SP 800-152; \\nFedRAMP standard contract \\nclauses; Cloud Computing \\nContract Best Practices; OMB \\nM-19-03; OMB A-130; CSF: \\nID.SC-2 through 4, NIST IR \\n8276). \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Supply Chain Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization ensures that \\nits policies, procedures, and \\nprocesses are consistently \\nimplemented for assessing and \\nreviewing the supply chain-\\nrelated risks associated with \\nsuppliers or contractors and \\nthe system, system \\ncomponent. \\n\\nThe organization uses \\nqualitative and quantitative \\nperformance metrics (e.g., \\nthose defined within SLAs) to \\nmeasure, report on, and monitor \\nthe information security and \\nSCRM performance of \\norganizationally defined \\nproducts, systems, and services \\nprovided by external providers. \\n\\nThe organization analyzes, in a \\nnear-real time basis, the impact \\nof material changes to security \\nand SCRM assurance \\nrequirements on its \\nrelationships with external \\nproviders and ensures that \\nacquisition tools, methods, and \\nprocesses are updated as soon \\nas possible. \\n\\nIn addition, the organization \\nhas incorporated supplier risk \\nevaluations, based on \\ncriticality, into its continuous \\nmonitoring practices to \\nmaintain situational awareness \\ninto the supply chain risks. \\n\\nIn addition, the organization \\nobtains sufficient assurance, \\nthrough audits, test results, or \\nother forms of evaluation, that \\nthe security and supply chain \\ncontrols of systems or services \\nprovided by contractors or \\nother entities on behalf of the \\norganization meet FISMA \\nrequirements, OMB policy, \\nand applicable NIST guidance. \\n\\nFurthermore, the organization \\nmaintains visibility into its \\nupstream suppliers and can \\nconsistently track changes in \\nsuppliers.  \\n, \\n\\nThe organization has not \\ndefined and communicated \\npolicies, procedures, and \\nprocesses to ensure that \\n[organizationally defined \\nproducts, system components, \\nsystems, and services] adhere \\nto its cybersecurity and supply \\nchain risk management \\nrequirements. \\n\\nThe organization has defined \\nand communicated policies \\nand procedures to ensure that \\n[organizationally defined \\nproducts, system components, \\nsystems, and services] adhere \\nto its cybersecurity and supply \\nchain risk management \\nrequirements. The following \\ncomponents, at a minimum, \\nare defined  \\n-  The identification and \\n\\n- \\n\\nprioritization of externally \\nprovided systems, system \\ncomponents, and services \\nas well how the \\norganization maintains \\nawareness of its upstream \\nsuppliers \\nIntegration of acquisition \\nprocesses, including the \\nuse of contractual \\nagreements that stipulate \\nappropriate cyber and \\nSCRM measures for \\nexternal providers. \\n-  Tools and techniques to \\nutilize the acquisition \\nprocess to protect the \\nsupply chain, including, \\nrisk-based processes for \\nevaluating cyber supply \\nchain risks associated \\nwith third party providers, \\nas appropriate.  \\n\\n-  Contract tools or \\n\\nprocurement methods to \\nconfirm contractors are \\nmeeting their contractual \\nSCRM obligations. \\n\\nPage 25 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nIdentify Function Area (Supply Chain Risk Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined and communicated its \\ncomponent authenticity \\npolicies and procedures. \\n\\nThe organization has defined \\nand communicated its \\ncomponent authenticity \\npolicies and procedures.  At a \\nminimum the following areas \\nare addressed:   \\n\\nThe organization consistently \\nimplements its component \\nauthenticity policies and \\nprocedures.  \\n\\nFurther, the organization: \\n\\n- Procedures to detect and \\nprevent counterfeit components \\nfrom entering the system. \\n\\n-Provides component \\nauthenticity/anti-counterfeit \\ntraining for designated \\npersonnel.   \\n\\n- Procedures to maintain \\nconfiguration control over \\norganizationally defined \\nsystem components that are \\nawaiting repair and service or \\nrepaired components awaiting \\nreturn to service. \\n\\n- Requirements and procedures \\nfor reporting counterfeit system \\ncomponents \\n\\n-Maintains configuration \\ncontrol over organizationally \\ndefined system components \\nthat are awaiting repair and \\nservice or repaired \\ncomponents awaiting return to \\nservice.   \\n\\nIn a near real-time basis, the \\norganization can update its \\nsupply chain risk management \\npolicies and procedures, as \\nappropriate, to respond to \\nevolving and sophisticated \\nthreats. \\n\\nThe organization monitors, \\nanalyzes, and reports on the \\nqualitative and quantitative \\nperformance measures used to \\ngauge the effectiveness of its \\ncomponent authenticity policies \\nand procedures and ensures that \\ndata supporting the metrics is \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nIn addition, the organization \\nhas incorporated component \\nauthenticity controls into its \\ncontinuous monitoring \\npractices. \\n\\nQuestion \\n\\n15.  To what extent does the \\norganization ensure that \\ncounterfeit components are \\ndetected and prevented from \\nentering the organization’s \\nsystems? (800-53 rev 5 SR-11, \\n11 (1), and 11(2) \\n\\n16.  Provide any additional \\ninformation on the \\neffectiveness (positive or \\nnegative) of the organization’s \\nsupply chain risk management \\nprogram that was not noted in \\nthe questions above. Taking \\ninto consideration the overall \\nmaturity level generated from \\nthe questions above and based \\non all testing performed, is the \\nrisk management program \\neffective? \\n\\nPage 26 of 60 \\n\\n \\n \\n \\n\\x0cPROTECT FUNCTION AREA \\nTable 7: Configuration Management \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nMaturity Level \\n\\nQuestion \\n\\n17.  To what extent have the roles \\nand responsibilities of \\nconfiguration management \\nstakeholders been defined, \\ncommunicated, and \\nimplemented across the agency, \\nand appropriately resourced \\n(NIST SP 800-53 REV. 4: CM-\\n1; NIST SP 800-128: Section \\n2.4)? \\n\\n18.  To what extent does the \\n\\norganization utilize an enterprise \\nwide configuration management \\nplan that includes, at a \\nminimum, the following \\ncomponents: roles and \\nresponsibilities, including \\nestablishment of a Change \\nControl Board (CCB) or related \\nbody; configuration management \\nprocesses, including processes \\nfor: identifying and managing \\nconfiguration items during the \\nappropriate phase within an \\norganization’s SDLC; \\nconfiguration monitoring; and \\napplying configuration \\nmanagement requirements to \\ncontractor operated systems \\n(NIST SP 800-128: Section \\n2.3.2; NIST SP 800-53 REV. 4: \\nCM-9)? \\n\\nAd Hoc \\n\\nDefined \\n\\nRoles and responsibilities at \\nthe organizational and \\ninformation system levels for \\nstakeholders involved in \\ninformation system \\nconfiguration management \\nhave not been fully defined \\nand communicated across the \\norganization. \\n\\nRoles and responsibilities at \\nthe organizational and \\ninformation system levels for \\nstakeholders involved in \\ninformation system \\nconfiguration management \\nhave been fully defined and \\ncommunicated across the \\norganization.  \\n\\nThe organization has not \\ndeveloped an organization \\nwide configuration \\nmanagement plan with the \\nnecessary components. \\n\\nThe organization has \\ndeveloped an organization \\nwide configuration \\nmanagement plan that includes \\nthe necessary components.  \\n\\nIndividuals are performing \\nthe roles and responsibilities \\nthat have been defined \\nacross the organization. \\n\\nConsistently  Implemented  Managed and Measurable \\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to effectively \\nperform information system \\nconfiguration management \\nactivities. Further, \\nstakeholders are held \\naccountable for carrying out \\ntheir roles and responsibilities \\neffectively. \\nThe organization monitors, \\nanalyzes, and reports to \\nstakeholders qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its configuration \\nmanagement plan, uses this \\ninformation to take corrective \\nactions when necessary, and \\nensures that data supporting \\nthe metrics is obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nThe organization has \\nconsistently implemented an \\norganization wide \\nconfiguration management \\nplan and has integrated its plan \\nwith its risk management and \\ncontinuous monitoring \\nprograms. Further, the \\norganization utilizes lessons \\nlearned in implementation to \\nmake improvements to its \\nplan. \\n\\nOptimized \\n\\nThe organization utilizes \\nautomation to adapt its \\nconfiguration management \\nplan and related processes and \\nactivities to a changing \\ncybersecurity landscape on a \\nnear real-time basis (as \\ndefined by the organization). \\n\\nPage 27 of 60 \\n\\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nQuestion \\n\\n19.  To what extent does the \\n\\norganization utilize baseline \\nconfigurations for its \\ninformation systems and \\nmaintain inventories of related \\ncomponents at a level of \\ngranularity necessary for \\ntracking and reporting (NIST SP \\n800-53 REV. 4: CM-2 and CM-\\n8; FY 2021 CIO FISMA \\nMetrics: 2.2, 3.9.2, and 3.10.1; \\nCSF: DE.CM-7 and PR.IP-1)? \\n\\n20.  To what extent does the \\norganization utilize \\nconfiguration settings/common \\nsecure configurations for its \\ninformation systems? (NIST SP \\n800-53 REV. 4: CM-6, CM-7, \\nRA-5, and SI-2; NIST SP 800-\\n70, Rev. 4, FY 2021 CIO \\nFISMA Metrics: 2.1, 2.2, 4.3; \\nSANS/CIS Top 20 Security \\nControls 3.7; CSF: ID.RA-1 and \\nDE.CM-8)? \\n\\nAd Hoc \\n\\nThe organization has not \\nestablished policies and \\nprocedures to ensure that \\nbaseline configurations for \\nits information systems are \\ndeveloped, documented, \\nand maintained under \\nconfiguration control and \\nthat system components \\nare inventoried at a level \\nof granularity deemed \\nnecessary for tracking and \\nreporting. \\n\\nThe organization has not \\nestablished policies and \\nprocedures for ensuring \\nthat configuration \\nsettings/common secure \\nconfigurations are defined, \\nimplemented, and \\nmonitored. \\n\\nDefined \\nThe organization has \\ndeveloped, documented, and \\ndisseminated its baseline \\nconfiguration and component \\ninventory policies and \\nprocedures. \\n\\nThe organization has \\ndeveloped, documented, and \\ndisseminated its policies and \\nprocedures for configuration \\nsettings/common secure \\nconfigurations. In addition, the \\norganization has developed, \\ndocumented, and disseminated \\ncommon secure configurations \\n(hardening guides) that are \\ntailored to its environment. \\nFurther, the organization has \\nestablished a deviation \\nprocess. \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nThe organization employs \\nautomated mechanisms (such \\nas application whitelisting \\nand network management \\ntools) to detect unauthorized \\nhardware, software, and \\nfirmware and unauthorized \\nchanges to hardware, \\nsoftware, and firmware. \\n\\nThe organization utilizes \\ntechnology to implement a \\ncentralized baseline \\nconfiguration and information \\nsystem component inventory \\nprocess that includes \\ninformation from all \\norganization systems \\n(hardware and software) and is \\nupdated in a near real-time \\nbasis. \\n\\nThe organization deploys \\nsystem configuration \\nmanagement tools that \\nautomatically enforce and \\nredeploy configuration settings \\nto systems at frequent intervals \\nas defined by the organization, \\nor on an event driven basis. \\n\\nThe organization employs \\nautomation to help maintain \\nan up-to-date, complete, \\naccurate, and readily \\navailable view of the security \\nconfigurations for all \\ninformation system \\ncomponents connected to the \\norganization’s network and \\nmakes appropriate \\nmodifications in accordance \\nwith organization-defined \\ntimelines. \\n\\nThe organization consistently \\nrecords, implements, and \\nmaintains under configuration \\ncontrol, baseline \\nconfigurations of its \\ninformation systems and an \\ninventory of related \\ncomponents in accordance \\nwith the organization's policies \\nand procedures. Further, the \\norganization utilizes lessons \\nlearned in implementation to \\nmake improvements to its \\nbaseline configuration policies \\nand procedures. \\nThe organization \\nconsistently implements, \\nassesses, and maintains \\nsecure configuration \\nsettings for its information \\nsystems based on the \\nprinciple of least \\nfunctionality. \\n\\nFurther, the organization \\nconsistently utilizes SCAP-\\nvalidated software assessing \\n(scanning) capabilities against \\nall systems on the network \\n(see inventory from questions \\n#1 - #3) to assess and manage \\nboth code-based and \\nconfiguration-based \\nvulnerabilities. The \\norganization utilizes lessons \\nlearned in implementation to \\nmake improvements to its \\nsecure configuration policies \\nand procedures \\n\\nPage 28 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0cQuestion \\n\\n21.  To what extent does the \\norganization utilize flaw \\nremediation processes, including \\npatch management, to manage \\nsoftware vulnerabilities (NIST \\nSP 800-53 REV. 4: CM-3, RA-\\n5, SI-2, and SI-3; NIST SP 800-\\n40, Rev. 3; SANS/CIS Top 20, \\nControl 4.5; FY 2021 CIO \\nFISMA Metrics: 1.3.7, 1.3.8, \\n2.13, 2.14; CSF: ID.RA-1; DHS \\nBinding Operational Directives \\n(BOD) 18-02 and 19-02)? \\n\\nAd Hoc \\n\\nThe organization has not \\ndeveloped, documented, \\nand disseminated its \\npolicies and procedures for \\nflaw remediation, \\nincluding for mobile \\ndevices (GFE and non- \\nGFE). \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nDefined \\nThe organization has \\ndeveloped, documented, and \\ndisseminated its policies and \\nprocedures for flaw \\nremediation, including for \\nmobile devices. Policies and \\nprocedures include processes \\nfor: identifying, reporting, and \\ncorrecting information system \\nflaws, testing software and \\nfirmware updates prior to \\nimplementation, installing \\nsecurity relevant updates and \\npatches within organizational-\\ndefined timeframes, and \\nincorporating flaw remediation \\ninto the organization's \\nconfiguration management \\nprocesses. \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nThe organization \\nconsistently implements its \\nflaw remediation policies, \\nprocedures, and processes \\nand ensures that patches, \\nhotfixes, service packs, and \\nanti-virus/malware \\nsoftware updates are \\nidentified, prioritized, \\ntested, and installed in a \\ntimely manner. In addition, \\nthe organization patches \\ncritical vulnerabilities \\nwithin 30 days and utilizes \\nlessons learned in \\nimplementation to make \\nimprovements to its flaw \\nremediation policies and \\nprocedures. \\n\\nThe organization centrally \\nmanages its flaw remediation \\nprocess and utilizes \\nautomated patch management \\nand software update tools for \\noperating systems, where \\nsuch tools are available and \\nsafe. \\n\\nThe organization utilizes \\nautomated patch management \\nand software update tools for \\nall applications and network \\ndevices (including mobile \\ndevices), as appropriate, where \\nsuch tools are available and \\nsafe. \\n\\nAs part its flaw remediation \\nprocesses, the organization \\nperforms deeper analysis of \\nsoftware code, such as through \\npatch sourcing and testing  \\n\\nThe organization monitors, \\nanalyzes, and reports \\nqualitative and quantitative \\nperformance measures on the \\neffectiveness of flaw \\nremediation processes and \\nensures that data supporting \\nthe metrics is obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nPage 29 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nQuestion \\n\\nAd Hoc \\n\\n22.  To what extent has the \\n\\norganization adopted the Trusted \\nInternet Connection (TIC) \\nprogram to assist in protecting \\nits network (OMB M-19-26, \\nDHS-CISA TIC 3.0 Core \\nGuidance Documents)  \\n\\nThe organization has not \\nprepared and planned to meet \\nthe goals of the TIC initiative, \\nconsistent with OMB M-19-26.  \\nSpecifically, the agency has not \\ndefined and customized, as \\nappropriate, its policies, \\nprocedures, and processes to \\nimplement TIC 3.0, including \\nupdating its network and \\nsystem boundary policies, in \\naccordance with OMB M-19-\\n26. This includes, as \\nappropriate, the TIC security \\ncapabilities catalog, TIC use \\ncases, and TIC overlays.   \\n\\nDefined \\nThe organization has prepared \\nand planned to meet the goals \\nof the TIC initiative, consistent \\nwith OMB M-19-26. \\nSpecifically, the agency has \\ndefined and customized, as \\nappropriate, its policies, \\nprocedures, and processes to \\nimplement TIC 3.0, including \\nupdating its network and \\nsystem boundary policies, in \\naccordance with OMB M-19-\\n26. This includes, as \\nappropriate, incorporation of \\nTIC security capabilities \\ncatalog, TIC use cases, and TIC \\noverlays.   \\n\\nThe agency has not \\ndefined processes to \\ndevelop and maintain an \\naccurate inventory of its \\nnetwork connections, \\nincluding details on the \\nservice provider, cost, \\ncapacity, traffic volume, \\nlogical/physical \\nconfigurations, and \\ntopological data for each \\nconnection. \\n\\nThe agency has defined \\nprocesses to develop and \\nmaintain an accurate inventory \\nof its network connections, \\nincluding details on the service \\nprovider, cost, capacity, traffic \\nvolume, logical/physical \\nconfigurations, and topological \\ndata for each connection. \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nThe organization \\nconsistently implements \\nTIC requirements based on \\nOMB M-19-26. This \\nincludes consistent \\nimplementation of defined \\nTIC security controls, as \\nappropriate, and ensuring \\nthat that all agency traffic, \\nincluding mobile and \\ncloud, are routed through \\ndefined access points, as \\nappropriate. \\n\\nThe agency develops and \\nmaintains an accurate \\ninventory of agency network \\nconnections, including details \\non the service provider, cost, \\ncapacity, traffic volume, \\nlogical/physical \\nconfigurations, and topological \\ndata for each connection.  \\n\\nThe organization, in \\naccordance with OMB M-\\n19-26, DHS guidance, and \\nits cloud strategy is \\nensuring that its TIC \\nimplementation remains \\nflexible and that its \\npolicies, procedures, and \\ninformation security \\nprogram are adapting to \\nmeet the security \\ncapabilities outlined in the \\nTIC initiative, consistent \\nwith OMB M-19-26. \\n\\nThe organization monitors \\nand reviews the \\nimplemented TIC 3.0 use \\ncases to determine \\neffectiveness and \\nincorporates new/different \\nuse cases, as appropriate.  \\n\\nPage 30 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nAd Hoc \\n\\nThe organization has not \\ndeveloped, documented, \\nand disseminated its \\npolicies and procedures for \\nmanaging configuration \\nchange control. Policies \\nand procedures do not \\naddress, at a minimum, the \\nnecessary configuration \\nchange control related \\nactivities. \\n\\nDefined \\nThe organization has \\ndeveloped, documented, and \\ndisseminated its policies and \\nprocedures for managing \\nconfiguration change control. \\nThe policies and procedures \\naddress, at a minimum, the \\nnecessary configuration \\nchange control related \\nactivities.  \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nThe organization \\nconsistently implements its \\nchange control policies, \\nprocedures, and processes, \\nincluding explicit \\nconsideration of security \\nimpacts prior to change \\nimplementation. \\n\\nThe organization utilizes \\nlessons learned in \\nimplementation to make \\nimprovements to its \\nchange control policies and \\nprocedures \\n\\nThe organization monitors, \\nanalyzes, and reports \\nqualitative and quantitative \\nperformance measures on the \\neffectiveness of its change \\ncontrol activities and ensures \\nthat data supporting the \\nmetrics is obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nIn addition, the organization \\nimplements [organizationally \\ndefined security responses] if \\nbaseline configurations are \\nchanged in an unauthorized \\nmanner. \\n\\nThe organization utilizes \\nautomation to improve the \\naccuracy, consistency, and \\navailability of configuration \\nchange control and \\nconfiguration baseline \\ninformation. Automation is \\nalso used to provide data \\naggregation and correlation \\ncapabilities, alerting \\nmechanisms, and dashboards \\non change control activities to \\nsupport risk-based decision \\nmaking across the \\norganization. \\n\\nQuestion \\n\\n23.  To what extent has the \\n\\norganization defined and \\nimplemented configuration \\nchange control activities \\nincluding: determination of the \\ntypes of changes that are \\nconfiguration controlled; review \\nand approval/disapproval of \\nproposed changes with explicit \\nconsideration of security impacts \\nand security classification of the \\nsystem; documentation of \\nconfiguration change decisions; \\nimplementation of approved \\nconfiguration changes; retaining \\nrecords of implemented changes; \\nauditing and review of \\nconfiguration changes; and \\ncoordination and oversight of \\nchanges by the CCB, as \\nappropriate (NIST SP 800-53 \\nREV. 4: CM-2, CM-3 and CM-\\n4; CSF: PR.IP-3). \\n\\nPage 31 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Configuration Management) \\n\\nDefined \\nThe organization has \\ndeveloped, documented, and \\npublicly disseminated a \\ncomprehensive VDP. The \\nfollowing elements are \\naddressed: \\n-  The systems in scope \\n-  Types of testing allowed \\n-  Reporting mechanisms \\n-  Timely feedback \\n-  Remediation \\n\\nIn addition, the organization \\nhas updated its vulnerability \\ndisclosure handling procedures \\nto support the implementation \\nof its VDP. \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nThe organization monitors, \\nanalyzes, and reports on the \\nqualitative and quantitative \\nperformance measures used \\nto gauge the effectiveness of \\nits vulnerability disclosure \\npolicy and disclosure handing \\nprocedures. \\n\\nIn addition, all internet-\\naccessible systems are \\nincluded in the scope of the \\norganization’s VDP. \\n\\nThe organization \\nconsistently implements its \\nVDP. In addition, the \\norganization:       \\n\\n- Has updated the relevant \\nfields at the .gov registrar \\nto ensure appropriate \\nreporting by the public.  \\n\\n-Ensures that newly \\nlaunched internet \\naccessible systems and \\nservices, and at least 50% \\nof internet-accessible \\nsystems, are included in \\nthe scope of its VDP. \\n\\n-Increases the scope of \\nsystems covered by its \\nVDP, in accordance with \\nDHS BOD 20-01. \\n\\nOptimized \\nOn a near real-time basis, the \\norganization actively adapts its \\nvulnerability disclosure \\npolicies and procedures and \\nprovides information to \\nstakeholders and partners.  \\n\\nWithin the context of its \\nenterprise risk management \\nprogram, the organization \\nconsiders the use of a Bug \\nBounty program. As \\nappropriate, Bug Bounty \\nprograms are implemented in \\naccordance with OMB M-20-\\n32. \\n\\nQuestion \\n\\n24.  To what extent does the \\n\\norganization utilize a \\nvulnerability disclosure policy \\n(VDP) as part of its vulnerability \\nmanagement program for \\ninternet-accessible federal \\nsystems (OMB M-20-32 and \\nDHS BOD 20-01)? \\n\\nAd Hoc \\n\\nThe organization has not \\ndeveloped, documented, \\nand disseminated a \\ncomprehensive VDP. \\n\\n25.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s configuration \\nmanagement program that was \\nnot noted in the questions above. \\nTaking into consideration the \\nmaturity level generated from \\nthe questions above and based \\non all testing performed, is the \\nconfiguration management \\nprogram effective? \\n\\nPage 32 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nTable 8: Identity and Access Management \\n\\nQuestion \\n\\n26.  To what extent have the roles and \\nresponsibilities of identity, \\ncredential, and access \\nmanagement (ICAM) \\nstakeholders been defined, \\ncommunicated, and implemented \\nacross the agency, and \\nappropriately resourced (NIST SP \\n800-53 REV. 4: AC-1, IA-1, and \\nPS-1; NIST SP 800-63-3 and 800-\\n63A, B, and C; Federal Identity, \\nCredential, and Access \\nManagement (FICAM) playbooks \\nand guidance (see \\nidmanagement.gov), OMB M-19-\\n17)? \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nRoles and responsibilities at \\nthe organizational and \\ninformation system levels for \\nstakeholders involved in \\nICAM have not been fully \\ndefined and communicated \\nacross the organization. \\n\\nRoles and responsibilities at \\nthe organizational and \\ninformation system levels for \\nstakeholders involved in \\nICAM have been fully defined \\nand communicated across the \\norganization. This includes, as \\nappropriate, developing an \\nICAM governance structure to \\nalign and consolidate the \\nagency’s ICAM investments, \\nmonitor programs, and \\nensuring awareness and \\nunderstanding.  \\n\\nIndividuals are performing \\nthe roles and responsibilities \\nthat have been defined \\nacross the organization.  \\n\\nThe organization ensures \\nthat there is consistent \\ncoordination amongst \\norganization leaders and \\nmission owners to \\nimplement, manage, and \\nmaintain the organization’s \\nICAM policy, strategy, \\nprocess, and technology \\nsolution roadmap. \\n\\nResources (people, \\nprocesses, and technology) \\nare allocated in a risk-based \\nmanner for stakeholders to \\neffectively implement \\nidentity, credential, and \\naccess management \\nactivities. Further, \\nstakeholders are held \\naccountable for carrying out \\ntheir roles and \\nresponsibilities effectively. \\n\\nIn accordance with OMB \\nM-19-17, the agency has \\nimplemented an integrated \\nagency-wide ICAM office, \\nteam, or other governance \\nstructure in support of its \\nERM capability to \\neffectively govern and \\nenforce ICAM efforts. \\n\\nPage 33 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nQuestion \\n\\n27.  To what extent does the \\n\\norganization utilize a \\ncomprehensive ICAM policy, \\nstrategy, process, and technology \\nsolution roadmap to guide its \\nICAM processes and activities \\n(FICAM, OMB M-19-17; NIST \\nSP 800-53 REV. 4: AC-1 and \\nIA-1; OMB M-19-17; \\nSANS/CIS Top 20: 14.1; DHS \\nED 19-01; CSF: PR.AC-4 and \\n5)? \\n\\nAd Hoc \\n\\nThe organization has not \\ndeveloped a comprehensive \\nICAM policy, strategy, \\nprocess, and technology \\nsolution road map to guide its \\nICAM processes and activities. \\n\\nDefined \\nThe organization has developed \\na comprehensive ICAM policy, \\nstrategy, process, and \\ntechnology solution road map \\nto guide its ICAM processes \\nand activities.  \\n\\nIn addition, the organization \\nhas not performed a review of \\ncurrent practices, identified \\ngaps, and developed a \\ntransition plan to serve as an \\ninput to the ICAM policy, \\nstrategy, and technology \\nsolution road map. \\n\\nThe organization has developed \\nmilestones for how it plans to \\nalign with Federal initiatives, \\nincluding strong authentication, \\nthe Federal ICAM architecture \\nand OMB M-19-17, and phase \\n2 of DHS's Continuous \\nDiagnostics and Mitigation \\n(CDM) program, as \\nappropriate. \\n\\nMaturity Level \\n\\nConsistently Implemented  Managed and Measurable \\n\\nOptimized \\n\\nThe organization is \\nconsistently implementing \\nits ICAM policy, strategy, \\nprocess, and technology \\nsolution road map and is on \\ntrack to meet milestones. \\nThe strategy encompasses \\nthe entire organization, \\naligns with the FICAM and \\nCDM requirements, and \\nincorporates applicable \\nFederal policies, standards, \\nplaybooks, and guidelines. \\n\\nFurther, the organization is \\nconsistently capturing and \\nsharing lessons learned on \\nthe effectiveness of its \\nICAM policy, strategy, and \\nroad map and making \\nupdates as needed.  \\n\\nOn a near real-time \\nbasis, the organization \\nactively adapts its ICAM \\npolicy, strategy, and \\nrelated processes and \\nactivities to a changing \\ncybersecurity landscape \\nto respond to evolving \\nand sophisticated \\nthreats. \\n\\nThe organization \\nemploys adaptive \\nidentification and \\nauthentication \\ntechniques to assess \\nsuspicious behavior and \\npotential violations of its \\nICAM policies and \\nprocedures on a near-\\nreal time basis. \\n\\nThe organization integrates \\nits ICAM strategy and \\nactivities with its enterprise \\narchitecture and the Federal \\nICAM architecture.  \\n\\nThe organization uses \\nautomated mechanisms \\n(e.g. machine-based, or \\nuser-based enforcement), \\nwhere appropriate, to \\nmanage the effective \\nimplementation of its \\nICAM policies, procedures, \\nand strategy. Examples of \\nautomated mechanisms \\ninclude network \\nsegmentation based on the \\nlabel/classification of \\ninformation stored; \\nautomatic \\nremoval/disabling of \\ntemporary/emergency/ \\ninactive accounts; and use \\nof automated tools to \\ninventory and manage \\naccounts and perform \\nsegregation of duties/least \\nprivilege reviews. \\n\\nPage 34 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nQuestion \\n\\n28.  To what extent has the \\n\\norganization developed and \\nimplemented processes for \\nassigning position risk \\ndesignations and performing \\nappropriate personnel screening \\nprior to granting access to its \\nsystems (NIST SP 800-53 REV. \\n4: PS-2 and PS-3; National \\nInsider Threat Policy; CSF: \\nPR.IP-11, OMB M-19-17)? \\n\\n29.  To what extent does the \\n\\norganization ensure that access \\nagreements, including \\nnondisclosure agreements, \\nacceptable use agreements, and \\nrules of behavior, as appropriate, \\nfor individuals (both privileged \\nand non-privileged users) that \\naccess its systems are completed \\nand maintained (NIST SP 800-\\n53 REV. 4: AC-8, PL-4, and PS-\\n6)? \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its processes for \\nassigning personnel risk \\ndesignations and performing \\nappropriate screening prior to \\ngranting access to its systems. \\n\\nThe organization has not \\ndefined its processes for \\ndeveloping, documenting, and \\nmaintaining access agreements \\nfor individuals that access its \\nsystems. \\n\\nThe organization has defined \\nits processes for ensuring that \\nall personnel are assigned risk \\ndesignations and appropriately \\nscreened prior to being granted \\naccess to its systems. Processes \\nhave been defined for assigning \\nrisk designations for all \\npositions, establishing \\nscreening criteria for \\nindividuals filling those \\npositions, authorizing access \\nfollowing screening \\ncompletion, and rescreening \\nindividuals on a periodic basis.  \\nThe organization has defined \\nits processes for developing, \\ndocumenting, and maintaining \\naccess agreements for \\nindividuals that access its \\nsystems.  \\n\\nThe organization ensures \\nthat all personnel are \\nassigned risk designations, \\nappropriately screened \\nprior to being granted \\nsystem access, and \\nrescreened periodically. \\n\\nThe organization employs \\nautomation to centrally \\ndocument, track, and share \\nrisk designations and \\nscreening information with \\nnecessary parties. \\n\\nOn a near-real time \\nbasis, the organization \\nevaluates personnel \\nsecurity information \\nfrom various sources, \\nintegrates this \\ninformation with \\nanomalous user behavior \\ndata (audit logging) \\nand/or its insider threat \\nactivities, and adjusts \\npermissions accordingly. \\n\\nThe organization uses \\nautomation to manage and \\nreview user access \\nagreements for privileged \\nand non-privileged users. \\nTo the extent practical, this \\nprocess is centralized.  \\n\\nOn a near real-time \\nbasis, the organization \\nensures that access \\nagreements for \\nprivileged and non-\\nprivileged users are \\nmaintained, as \\nnecessary. \\n\\nThe organization ensures \\nthat access agreements for \\nindividuals are completed \\nprior to access being \\ngranted to systems and are \\nconsistently maintained \\nthereafter. The \\norganization utilizes more \\nspecific/detailed \\nagreements for privileged \\nusers or those with access \\nto sensitive information, as \\nappropriate. \\n\\nPage 35 of 60 \\n\\n \\n \\n\\x0cQuestion \\n\\n30.  To what extent has the \\n\\norganization implemented strong \\nauthentication mechanisms (PIV \\nor an Identity Assurance Level \\n(IAL)3/Authenticator Assurance \\nLevel (AAL) 3 credential) for \\nnon-privileged users to access \\nthe organization's facilities \\n[organization-defined entry/exit \\npoints], networks, and systems, \\nincluding for remote access \\n(HSPD-12; NIST SP 800-53 \\nREV. 4: AC-17, IA-2, IA-5, IA-\\n8, and PE-3; NIST SP 800-128; \\nFIPS 201-2; NIST SP 800-63, \\n800-157; FY 2021 CIO FISMA \\nMetrics: 2.4, 2.7, CSF: PR.AC-1 \\nand 6; OMB M-19-17, and  \\nNIST SP 800-157)? \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nMaturity Level \\n\\nAll non-privileged users \\nutilize strong authentication \\nmechanisms to authenticate \\nto applicable organizational \\nsystems and facilities \\n[organization-defined \\nentry/exit points]. \\n\\nThe organization has planned \\nfor the use of strong \\nauthentication mechanisms for \\nnon-privileged users of the \\norganization’s facilities \\n[organization-defined entry/exit \\npoints], systems, and networks, \\nincluding the completion of \\ndigital identity risk \\nassessments.  \\n\\nThe organization has not \\nplanned for the use of strong \\nauthentication mechanisms for \\nnon-privileged users of the \\norganization’s facilities \\n[organization-defined \\nentry/exit points], systems, and \\nnetworks, including for remote \\naccess. In addition, the \\norganization has not performed \\ndigital identity risk \\nassessments to determine \\nwhich systems require strong \\nauthentication. \\n\\nThe organization has \\nconsistently implemented \\nstrong authentication \\nmechanisms for non- \\nprivileged users of the \\norganization’s facilities \\n[organization-defined \\nentry/exit points] and \\nnetworks, including for \\nremote access, in \\naccordance with Federal \\ntargets. \\n\\nFor instances where it \\nwould be impracticable to \\nuse the PIV card, the \\norganization uses an \\nalternative token (derived \\nPIV credential) which can \\nbe implemented and \\ndeployed with mobile \\ndevices. \\n\\nOptimized \\nThe organization has \\nimplemented an \\nenterprise-wide single \\nsign on solution and all \\nof the organization's \\nsystems interface with \\nthe solution, resulting in \\nan ability to manage user \\n(non-privileged) \\naccounts and privileges \\ncentrally and report on \\neffectiveness on a near \\nreal-time basis. \\n\\nPage 36 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cQuestion \\n\\n31.  To what extent has the \\n\\norganization implemented strong \\nauthentication mechanisms (PIV \\nor an Identity Assurance Level \\n(IAL)3/Authenticator Assurance \\nLevel (AAL) 3 credential) for \\nprivileged users to access the \\norganization's facilities \\n[organization-defined entry/exit \\npoints], networks, and systems, \\nincluding for remote access \\n(HSPD-12; NIST SP 800-53 \\nREV. 4: AC-17, PE-3; NIST SP \\n800-128; FIPS 201-2; NIST SP \\n800-63 and 800-157; OMB  M-\\n19-17, FY 2021 CIO FISMA \\nMetrics: 2.3, 2.5, and 2.7; CSF: \\nPR.AC-1 and 6; and DHS ED \\n19-01)?  \\n\\nFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nMaturity Level \\n\\nThe organization has not \\nplanned for the use of strong \\nauthentication mechanisms for \\nprivileged users of the \\norganization’s facilities \\n[organization-defined \\nentry/exit points], systems, and \\nnetworks, including for remote \\naccess. In addition, the \\norganization has not performed \\ndigital identity risk \\nassessments to determine \\nwhich systems require strong \\nauthentication. \\n\\nThe organization has planned \\nfor the use of strong \\nauthentication mechanisms for \\nprivileged users of the \\norganization’s facilities \\n[organization-defined entry/exit \\npoints], systems, and networks, \\nincluding the completion of \\ndigital identity risk \\nassessments. \\n\\nThe organization has \\nconsistently implemented \\nstrong authentication \\nmechanisms for privileged \\nusers of the organization’s \\nfacilities [organization-\\ndefined entry/exit points], \\nand networks, including for \\nremote access, in \\naccordance with Federal \\ntargets. \\n\\nAll privileged users, \\nincluding those who can \\nmake changes to DNS \\nrecords, utilize strong \\nauthentication mechanisms \\nto authenticate to \\napplicable organizational \\nsystems. \\n\\nFor instances where it \\nwould be impracticable to \\nuse the PIV card, the \\norganization uses an \\nalternative token (derived \\nPIV credential) which can \\nbe implemented and \\ndeployed with mobile \\ndevices. \\n\\nOptimized \\nThe organization has \\nimplemented an \\nenterprise-wide single \\nsign on solution and all \\nthe organization's \\nsystems interface with \\nthe solution, resulting in \\nan ability to manage user \\n(privileged) accounts \\nand privileges centrally \\nand report on \\neffectiveness on a near \\nreal-time basis. \\n\\nPage 37 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its processes for \\nprovisioning, managing, and \\nreviewing privileged accounts. \\n\\nThe organization has defined \\nits processes for provisioning, \\nmanaging, and reviewing \\nprivileged accounts. Defined \\nprocesses cover approval and \\ntracking, inventorying and \\nvalidating, and logging and \\nreviewing privileged users' \\naccounts. \\n\\nThe organization employs \\nautomated mechanisms \\n(e.g. machine-based, or \\nuser-based enforcement) to \\nsupport the management of \\nprivileged accounts, \\nincluding for the automatic \\nremoval/disabling of \\ntemporary, emergency, and \\ninactive accounts, as \\nappropriate. \\n\\nThe organization ensures \\nthat its processes for \\nprovisioning, managing, \\nand reviewing privileged \\naccounts are consistently \\nimplemented across the \\norganization. The \\norganization limits the \\nfunctions that can be \\nperformed when using \\nprivileged accounts; limits \\nthe duration that privileged \\naccounts can be logged in; \\nlimits the privileged \\nfunctions that can be \\nperformed using remote \\naccess; and ensures that \\nprivileged user activities \\nare logged and periodically \\nreviewed. \\n\\nThe organization has not \\ndefined the \\nconfiguration/connection \\nrequirements for remote access \\nconnections, including use of \\nFIPS 140-2 validated \\ncryptographic modules, system \\ntime-outs, and monitoring and \\ncontrol of remote access \\nsessions. \\n\\nThe organization has defined \\nits configuration/connection \\nrequirements for remote access \\nconnections, including use of \\ncryptographic modules, system \\ntime-outs, and how it monitors \\nand controls remote access \\nsessions. \\n\\nThe organization ensures \\nthat FIPS 140-2 validated \\ncryptographic modules are \\nimplemented for its remote \\naccess connection \\nmethod(s), remote access \\nsessions time out after 30 \\nminutes (or less), and that \\nremote users' activities are \\nlogged and reviewed based \\non risk. \\n\\nThe organization ensures \\nthat end user devices have \\nbeen appropriately \\nconfigured prior to \\nallowing remote access and \\nrestricts the ability of \\nindividuals to transfer data \\naccessed remotely to non-\\nauthorized devices. \\n\\nThe organization has \\ndeployed a capability to \\nrapidly disconnect \\nremote access user \\nsessions based on active \\nmonitoring. The speed \\nof disablement varies \\nbased on the criticality \\nof missions/business \\nfunctions. \\n\\nQuestion \\n\\n32.  To what extent does the \\norganization ensure that \\nprivileged accounts are \\nprovisioned, managed, and \\nreviewed in accordance with the \\nprinciples of least privilege and \\nseparation of duties? \\nSpecifically, this includes \\nprocesses for periodic review \\nand adjustment of privileged \\nuser accounts and permissions, \\ninventorying and validating the \\nscope and number of privileged \\naccounts, and ensuring that \\nprivileged user account activities \\nare logged and periodically \\nreviewed (FY 2021 CIO FISMA \\nMetrics: 2.3, 2.5, 2.6, and 2.7; \\nOMB  M-19-17, NIST SP 800-\\n53 REV. 4: AC-1, AC-2, AC-5, \\nAC-6, AC-17; AU-2, AU-3, AU-\\n6, and IA-4; DHS ED 19-01; \\nCSF: PR.AC-4). \\n33.  To what extent does the \\norganization ensure that \\nappropriate \\nconfiguration/connection \\nrequirements are maintained for \\nremote access connections? This \\nincludes the use of appropriate \\ncryptographic modules, system \\ntime-outs, and the monitoring \\nand control of remote access \\nsessions (NIST SP 800-53 REV. \\n4: AC-11, AC-12, AC-17, AC-\\n19, AU-2, IA-7, SC-10,  SC-13, \\nand SI-4; CSF: PR.AC-3; and \\nFY 2021 CIO FISMA Metrics: \\n2.10 and 2.11). \\n\\nPage 38 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Reporting Metrics v1.1 \\nProtect Function Area (Identity and Access Management) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nQuestion \\n\\n34.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s identity and \\naccess management program that \\nwas not noted in the questions \\nabove. Taking into consideration \\nthe maturity level generated \\nfrom the questions above and \\nbased on all testing performed, is \\nthe identity and access \\nmanagement program effective? \\n\\nPage 39 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Data Protection and Privacy)  \\n\\nTable 9: Data Protection and Privacy \\n\\nQuestion \\n\\n35.  To what extent has the \\n\\norganization developed a privacy \\nprogram for the protection of \\npersonally identifiable \\ninformation (PII) that is collected, \\nused, maintained, shared, and \\ndisposed of by information \\nsystems (NIST SP 800-122; NIST \\nSP 800-37 (Rev. 2) Section 2.3, \\nTask P-1 ; OMB M-20-04; OMB \\nM-19-03; OMB A-130, Appendix \\nI; CSF: ID.GV-3; NIST SP 800-\\n53 REV. 4: AR-4 and Appendix J, \\nFY 2020 SAOP FISMA metrics, \\nSections 1 through 4, 5(b), NIST \\nPrivacy Framework)? \\n\\nAd Hoc \\n\\nDefined \\n\\nThe organization has not \\nestablished a privacy program \\nand related plans, policies, and \\nprocedures as appropriate for \\nthe protection of PII collected, \\nused, maintained, shared, and \\ndisposed of by information \\nsystems. Additionally, roles \\nand responsibilities for the \\neffective implementation of \\nthe organization’s privacy \\nprogram have not been \\ndefined. \\n\\nThe organization has defined \\nand communicated its privacy \\nprogram plan and related \\npolicies and procedures for the \\nprotection of PII that is \\ncollected, used, maintained, \\nshared, and/or disposed of by \\nits information systems. In \\naddition, roles and \\nresponsibilities for the \\neffective implementation of \\nthe organization’s privacy \\nprogram have been defined \\nand the organization has \\ndetermined the resources and \\noptimal governance structure \\nneeded to effectively \\nimplement its privacy \\nprogram.  \\n\\nOptimized \\n\\nThe privacy program is fully \\nintegrated with other security \\nareas, such as ISCM, and other \\nbusiness processes, such as \\nstrategic planning and risk \\nmanagement. Further, the \\norganization's privacy program \\nis embedded into daily \\ndecision making across the \\norganization and provides for \\ncontinuous identification of \\nprivacy risks.  \\n\\nMaturity Level \\n\\nThe organization consistently \\nimplements its privacy \\nprogram by:  \\n\\nConsistently  Implemented  Managed and Measurable \\nThe organization monitors and \\nanalyses quantitative and \\nqualitative performance \\nmeasures on the effectiveness \\nof its privacy activities and \\nuses that information to make \\nneeded adjustments.  \\n\\n-  Dedicating appropriate \\n\\nresources to the program \\n\\nThe organization conducts an \\nindependent review of its \\nprivacy program and makes \\nnecessary improvements. \\n\\n-  Maintaining an inventory \\nof the collection and use \\nof PII  \\n\\n-  Conducting and \\n\\nmaintaining privacy \\nimpact assessments and \\nsystem of records notices \\nfor all applicable systems. \\n\\n-  Reviewing and removing \\n\\nunnecessary PII \\ncollections on a regular \\nbasis (i.e., SSNs) \\n\\n-  Using effective \\n\\ncommunications channels \\nfor disseminating privacy \\npolicies and procedures \\n\\n-  Ensuring that individuals \\n\\nare consistently \\nperforming the privacy \\nroles and responsibilities \\nthat have been defined \\nacross the organization \\n\\nPage 40 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cAd Hoc \\n\\nThe organization has not \\ndefined its policies and \\nprocedures in one or more of \\nthe specified areas. \\n\\nFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Data Protection and Privacy)  \\n\\nDefined \\nThe organization's policies and \\nprocedures have been defined \\nand communicated for the \\nspecified areas. Further, the \\npolicies and procedures have \\nbeen tailored to the \\norganization's environment \\nand include specific \\nconsiderations based on data \\nclassification and sensitivity. \\n\\nMaturity Level \\n\\nConsistently  Implemented  Managed and Measurable \\nThe organization ensures that \\nthe security controls for \\nprotecting PII and other \\nagency sensitive data, as \\nappropriate, throughout the \\ndata lifecycle are subject to the \\nmonitoring processes defined \\nwithin the organization's \\nISCM strategy. \\n\\nThe organization's policies and \\nprocedures have been \\nconsistently implemented for \\nthe specified areas, including \\n(i) use of FIPS-validated \\nencryption of PII and other \\nagency sensitive data, as \\nappropriate, both at rest and in \\ntransit, (ii) prevention and \\ndetection of untrusted \\nremovable media, and (iii) \\ndestruction or reuse of media \\ncontaining PII or other \\nsensitive agency data.  \\n\\nQuestion \\n\\n36.  To what extent has the \\n\\norganization implemented the \\nfollowing security controls to \\nprotect its PII and other agency \\nsensitive data, as appropriate, \\nthroughout the data lifecycle. \\n(NIST SP 800-53 REV. 4; \\nAppendix J, SC-8, SC-28, MP-3, \\nand MP-6; NIST SP 800-37 (Rev. \\n2); FY 2021 CIO FISMA Metrics: \\n2.8, 2.12; DHS BOD 18-02; CSF: \\nPR.DS-1, PR.DS-2, PR.PT-2, and \\nPR.IP-6)? \\n• \\n• \\n• \\n\\nEncryption of data at rest \\nEncryption of data in transit \\nLimitation of transfer to \\nremovable media \\nSanitization of digital media \\nprior to disposal or reuse \\n\\n• \\n\\n37.  To what extent has the \\n\\norganization implemented \\nsecurity controls to prevent data \\nexfiltration and enhance network \\ndefenses? (NIST SP 800-53 REV. \\n4: SI-3, SI-7(8), SI-4(4) and (18), \\nSC-7(10), and SC-18; FY 2021 \\nCIO FISMA Metrics: 3.8; DHS \\nBOD 18-01; DHS ED 19-01; \\nCSF: PR.DS-5)? \\n\\nThe organization has not \\ndefined its policies and \\nprocedures related to data \\nexfiltration, enhanced network \\ndefenses, email authentication \\nprocesses, and mitigation \\nagainst DNS infrastructure \\ntampering. \\n\\nThe organization has defined \\nand communicated it policies \\nand procedures for data \\nexfiltration, enhanced network \\ndefenses, email authentication \\nprocesses, and mitigation \\nagainst DNS infrastructure \\ntampering.  \\n\\nThe organization analyzes \\nqualitative and quantitative \\nmeasures on the performance \\nof its data exfiltration and \\nenhanced network defenses.  \\nThe organization also conducts \\nexfiltration exercises to \\nmeasure the effectiveness of \\nits data exfiltration and \\nenhanced network defenses. \\n\\nFurther, the organization \\nmonitors its DNS \\ninfrastructure for potential \\ntampering, in accordance with \\nits ISCM strategy. In addition, \\nthe organization audits its \\nDNS records. \\n\\nThe organization consistently \\nmonitors inbound and \\noutbound network traffic, \\nensuring that all traffic passes \\nthrough a web content filter \\nthat protects against phishing, \\nmalware, and blocks against \\nknown malicious sites. \\nAdditionally, the organization \\nchecks outbound \\ncommunications traffic to \\ndetect encrypted exfiltration of \\ninformation, anomalous traffic \\npatterns, and elements of PII. \\nAlso, suspected malicious \\ntraffic is quarantined or \\nblocked. \\n\\nIn addition, the organization \\nutilizes email authentication \\ntechnology and ensures the use \\nof valid encryption certificates \\nfor its domains. \\n\\nPage 41 of 60 \\n\\nOptimized \\n\\nThe organization employs \\nadvanced capabilities to \\nenhance protective controls, \\nincluding (i) remote wiping, \\n(ii) dual authorization for \\nsanitization of media devices, \\n(iii) exemption of media \\nmarking as long as the media \\nremains within \\norganizationally-defined \\ncontrol areas, and (iv) \\nconfiguring systems to record \\nthe date the PII was collected, \\ncreated, or updated and when \\nthe data is to be deleted or \\ndestroyed according to an \\napproved data retention \\nschedule. \\n\\nThe organizations data \\nexfiltration and enhanced \\nnetwork defenses are fully \\nintegrated into the ISCM and \\nincident response programs to \\nprovide near real-time \\nmonitoring of the data that is \\nentering and exiting the \\nnetwork, and other suspicious \\ninbound and outbound \\ncommunications.  \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Data Protection and Privacy)  \\n\\nQuestion \\n\\n38.  To what extent has the \\n\\norganization developed and \\nimplemented a Data Breach \\nResponse Plan, as appropriate, to \\nrespond to privacy events? (NIST \\nSP 800-122; NIST SP 800-53 \\nREV. 4: Appendix J, SE-2; FY \\n2020 SAOP FISMA metrics, \\nSection 12; OMB M-17-12; and \\nOMB M-17-25)? \\n\\nAd Hoc \\n\\nDefined \\n\\nThe organization has not \\ndeveloped a Data Breach \\nResponse Plan that includes \\nthe agency’s policies and \\nprocedures for reporting, \\ninvestigating, and managing a \\nprivacy-related breach. \\nFurther, the organization has \\nnot established a breach \\nresponse team that includes \\nthe appropriate agency \\nofficials. \\n\\nThe organization has defined \\nand communicated its Data \\nBreach Response Plan, \\nincluding its processes and \\nprocedures for data breach \\nnotification. Further, a breach \\nresponse team has been \\nestablished that includes the \\nappropriate agency officials.  \\n\\n39.  To what extent does the \\n\\norganization ensure that privacy \\nawareness training is provided to \\nall individuals, including role-\\nbased privacy training (NIST SP \\n800-53 REV. 4: AR-5, FY 2020 \\nSAOP FISMA Metrics, Sections 9 \\n10, and 11)?  \\n\\nThe organization has not \\ndefined its privacy awareness \\ntraining program based on \\norganizational requirements, \\nits mission, and the types of \\nPII that its users have access \\nto. In addition, the \\norganization has not developed \\nrole-based privacy training for \\nindividuals having \\nresponsibility for PII or \\nactivities involving PII. \\n\\nThe organization has defined \\nand communicated its privacy \\nawareness training program, \\nincluding requirements for \\nrole-based privacy awareness \\ntraining. Further, training has \\nbeen tailored to the \\norganization’s mission and \\nrisk environment. \\n\\n(Note: Privacy awareness \\ntraining topics should include, \\nas appropriate: responsibilities \\nunder the Privacy Act of 1974 \\nand E-Government Act of \\n2002, consequences for failing \\nto carry out responsibilities, \\nidentifying privacy risks, \\nmitigating privacy risks, and \\nreporting privacy incidents, \\ndata collections and use \\nrequirements) \\n\\nOptimized \\n\\nThe organization's Data \\nBreach Response plan is fully \\nintegrated with incident \\nresponse, risk management, \\ncontinuous monitoring, \\ncontinuity of operations, and \\nother mission/business areas, \\nas appropriate. Further the \\norganization employs \\nautomation to monitor for \\npotential privacy incidents and \\ntakes immediate action to \\nmitigate the incident and \\nprovide protection to the \\naffected individuals. \\n\\nThe organization has \\ninstitutionalized a process of \\ncontinuous improvement \\nincorporating advanced \\nprivacy training practices and \\ntechnologies. \\n\\nConsistently  Implemented  Managed and Measurable \\nThe organization monitors and \\nanalyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its Data Breach Response \\nPlan, as appropriate. The \\norganization ensures that data \\nsupporting metrics are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format.  \\n\\nMaturity Level \\n\\nThe organization consistently \\nimplements its Data Breach \\nResponse plan. Additionally, \\nthe breach response team \\nparticipates in table-top \\nexercises and uses lessons \\nlearned to make improvements \\nto the plan as appropriate. \\nFurther, the organization can \\nidentify the specific \\nindividuals affected by a \\nbreach, send notice to the \\naffected individuals, and \\nprovide those individuals with \\ncredit monitoring and repair \\nservices, as necessary.  \\nThe organization ensures that \\nall individuals receive basic \\nprivacy awareness training and \\nindividuals having \\nresponsibilities for PII or \\nactivities involving PII receive \\nrole-based privacy training at \\nleast annually. Additionally, \\nthe organization ensures that \\nindividuals certify acceptance \\nof responsibilities for privacy \\nrequirements at least annually. \\n\\nThe organization measures the \\neffectiveness of its privacy \\nawareness training program by \\nobtaining feedback on the \\ncontent of the training and \\nconducting targeted phishing \\nexercises for those with \\nresponsibility for PII. \\nAdditionally, the organization \\nmake updates to its program \\nbased on statutory, regulatory, \\nmission, program, business \\nprocess, information system \\nrequirements, and/or results \\nfrom monitoring and auditing.  \\n\\nPage 42 of 60 \\n\\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Data Protection and Privacy)  \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nQuestion \\n\\n40.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s data protection and \\nprivacy program that was not \\nnoted in the questions above. \\nTaking into consideration the \\nmaturity level generated from the \\nquestions above and based on all \\ntesting performed, is the data \\nprotection and privacy program \\neffective? \\n\\nPage 43 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Security Training) \\n\\nTable 10: Security Training \\n\\nQuestion \\n\\n41.  To what extent have the roles \\nand responsibilities of security \\nawareness and training program \\nstakeholders been defined, \\ncommunicated, and \\nimplemented across the agency, \\nand appropriately resourced? \\n(Note: this includes the roles \\nand responsibilities for the \\neffective establishment and \\nmaintenance of an organization \\nwide security awareness and \\ntraining program as well as the \\nawareness and training related \\nroles and responsibilities of \\nsystem users and those with \\nsignificant security \\nresponsibilities (NIST SP 800-\\n53 REV. 4: AT-1; and NIST SP \\n800-50). \\n\\n42.  To what extent does the \\n\\norganization utilize an \\nassessment of the skills, \\nknowledge, and abilities of its \\nworkforce to provide tailored \\nawareness and specialized \\nsecurity training within the \\nfunctional areas of: identify, \\nprotect, detect, respond, and \\nrecover (NIST SP 800-53 REV. \\n4: AT-2 and AT-3; NIST SP 800-\\n50: Section 3.2; Federal \\nCybersecurity Workforce \\nAssessment Act of 2015; \\nNational Cybersecurity \\nWorkforce Framework v1.0; \\nNIST SP 800-181; and \\nCIS/SANS Top 20: 17.1)? \\n\\nAd Hoc \\n\\nRoles and responsibilities \\nhave not been defined, \\ncommunicated across the \\norganization, and \\nappropriately resourced. \\n\\nDefined \\nRoles and responsibilities have \\nbeen defined and \\ncommunicated across the \\norganization and resource \\nrequirements have been \\nestablished.  \\n\\nOptimized \\n\\nMaturity Level \\n\\nIndividuals are performing \\nthe roles and responsibilities \\nthat have been defined across \\nthe organization. \\n\\nConsistently  Implemented  Managed and Measurable \\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to consistently \\nimplement security awareness \\nand training responsibilities. \\nFurther, stakeholders are held \\naccountable for carrying out \\ntheir roles and responsibilities \\neffectively. \\n\\nThe organization has not \\ndefined its processes for \\nassessing the knowledge, \\nskills, and abilities of its \\nworkforce. \\n\\nThe organization has defined \\nits processes for assessing the \\nknowledge, skills, and abilities \\nof its workforce to determine \\nits awareness and specialized \\ntraining needs and periodically \\nupdating its assessment to \\naccount for a changing risk \\nenvironment.  \\n\\nThe organization has \\nassessed the knowledge, \\nskills, and abilities of its \\nworkforce; tailored its \\nawareness and specialized \\ntraining; and has identified \\nits skill gaps. Further, the \\norganization periodically \\nupdates its assessment to \\naccount for a changing risk \\nenvironment. In addition, the \\nassessment serves as a key \\ninput to updating the \\norganization’s awareness and \\ntraining strategy/plans. \\n\\nThe organization has \\naddressed its identified \\nknowledge, skills, and \\nabilities gaps through \\ntraining or talent \\nacquisition.  \\n\\nThe organization’s personnel \\ncollectively possess a training \\nlevel such that the \\norganization can demonstrate \\nthat security incidents \\nresulting from personnel \\nactions or inactions are being \\nreduced over time. \\n\\nPage 44 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Security Training) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its security awareness \\nand training strategy/plan for \\ndeveloping, implementing, \\nand maintaining a security \\nawareness and training \\nprogram that is tailored to its \\nmission and risk environment. \\n\\nThe organization has defined \\nits security awareness and \\ntraining strategy/plan for \\ndeveloping, implementing, and \\nmaintaining a security \\nawareness and training \\nprogram that is tailored to its \\nmission and risk environment.  \\n\\nThe organization has \\nconsistently implemented its \\norganization-wide security \\nawareness and training \\nstrategy and plan. \\n\\nThe organization monitors \\nand analyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its security awareness and \\ntraining strategies and plans. \\nThe organization ensures that \\ndata supporting metrics are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nThe organization’s security \\nawareness and training \\nactivities are integrated across \\nother security-related domains. \\nFor instance, common risks \\nand control weaknesses, and \\nother outputs of the agency’s \\nrisk management and \\ncontinuous monitoring \\nactivities inform any updates \\nthat need to be made to the \\nsecurity awareness and \\ntraining program. \\n\\nQuestion \\n\\n43.  To what extent does the \\n\\norganization utilize a security \\nawareness and training \\nstrategy/plan that leverages its \\nskills assessment and is adapted \\nto its mission and risk \\nenvironment? (Note: the \\nstrategy/plan should include the \\nfollowing components: the \\nstructure of the awareness and \\ntraining program, priorities, \\nfunding, the goals of the \\nprogram, target audiences, types \\nof courses/material for each \\naudience, use of technologies \\n(such as email advisories, \\nintranet updates/wiki \\npages/social media, web based \\ntraining, phishing simulation \\ntools), frequency of training, and \\ndeployment methods (NIST SP \\n800-53 REV. 4: AT-1; NIST SP \\n800-50: Section 3; CSF: PR.AT-\\n1). \\n\\nPage 45 of 60 \\n\\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Security Training) \\n\\nQuestion \\n\\n44.  To what extent does the \\n\\norganization ensure that security \\nawareness training is provided to \\nall system users and is tailored \\nbased on its mission, risk \\nenvironment, and types of \\ninformation systems? (Note: \\nawareness training topics should \\ninclude, as appropriate: \\nconsideration of organizational \\npolicies, roles and \\nresponsibilities, secure e-mail, \\nbrowsing, and remote access \\npractices, mobile device \\nsecurity, secure use of social \\nmedia, phishing, malware, \\nphysical security, and security \\nincident reporting (NIST SP \\n800-53 REV. 4: AT-1, AT-2; FY \\n2021 CIO FISMA Metrics: 2.15; \\nNIST SP 800-50: 6.2; CSF: \\nPR.AT-2; SANS Top 20: 17.4).  \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its security awareness \\npolicies, procedures, and \\nrelated material based on its \\nmission, risk environment, \\nand the types of information \\nsystems that its users have \\naccess to. \\n\\nThe organization has defined \\nand tailored its security \\nawareness policies, procedures, \\nand related material and \\ndelivery methods based on \\nFISMA requirements, its, and \\nthe types of information \\nsystems that its users have \\naccess to.  \\n\\nIn addition, the organization \\nhas not defined its processes \\nfor ensuring that all \\ninformation system users are \\nprovided security awareness \\ntraining [within \\norganizationally defined \\ntimeframes] and periodically \\nthereafter. \\n\\nFurthermore, the organization \\nhas not defined its processes \\nfor evaluating and obtaining \\nfeedback on its security \\nawareness and training \\nprogram and using that \\ninformation to make \\ncontinuous improvements. \\n\\nIn addition, the organization \\nhas defined its processes for \\nensuring that all information \\nsystem users including \\ncontractors are provided \\nsecurity awareness training \\n[within organizationally \\ndefined timeframes] and \\nperiodically thereafter. \\n\\nFurthermore, the organization \\nhas defined its processes for \\nevaluating and obtaining \\nfeedback on its security \\nawareness and training \\nprogram and using that \\ninformation to make \\ncontinuous improvements. \\n\\nThe organization ensures that \\nits security awareness \\npolicies and procedures are \\nconsistently implemented.  \\n\\nThe organization ensures that \\nall appropriate users \\ncomplete the organization’s \\nsecurity awareness training \\n(or a comparable awareness \\ntraining for contractors) \\n[within organizationally \\ndefined timeframes] and \\nperiodically thereafter and \\nmaintains completion \\nrecords.  \\n\\nThe organization obtains \\nfeedback on its security \\nawareness and training \\nprogram and uses that \\ninformation to make \\nimprovements. \\n\\nThe organization measures \\nthe effectiveness of its \\nawareness program by, for \\nexample, conducting phishing \\nexercises and following up \\nwith additional awareness or \\ntraining, and/or disciplinary \\naction, as appropriate. \\n\\nThe organization monitors \\nand analyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its security awareness \\npolicies, procedures, and \\npractices. The organization \\nensures that data supporting \\nmetrics are obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nOptimized \\nThe organization has \\ninstitutionalized a process of \\ncontinuous improvement \\nincorporating advanced \\nsecurity awareness practices \\nand technologies. \\n\\nOn a near real-time basis (as \\ndetermined by the agency \\ngiven its threat environment), \\nthe organization actively \\nadapts its security awareness \\npolicies, procedures, processes \\nto a changing cybersecurity \\nlandscape and provides \\nawareness and training, as \\nappropriate, on evolving and \\nsophisticated threats. \\n\\nPage 46 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nProtect Function Area (Security Training) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined its security training \\npolicies, procedures, and \\nrelated materials based on its \\nmission, risk environment, \\nand the types of roles with \\nsignificant security \\nresponsibilities.  \\n\\nThe organization has defined \\nits security training policies, \\nprocedures, and related \\nmaterial based on FISMA \\nrequirements, its mission and \\nrisk environment, and the types \\nof roles with significant \\nsecurity responsibilities.  \\n\\nIn addition, the organization \\nhas not defined its processes \\nfor ensuring that personnel \\nwith significant security roles \\nand responsibilities are \\nprovided specialized security \\ntraining [within \\norganizationally defined \\ntimeframes] and periodically \\nthereafter. \\n\\nIn addition, the organization \\nhas defined its processes for \\nensuring that personnel with \\nassigned security roles and \\nresponsibilities are provided \\nspecialized security training \\n[within organizationally \\ndefined time frames] and \\nperiodically thereafter. \\n\\nThe organization ensures that \\nits security training policies \\nand procedures are \\nconsistently implemented.  \\n\\nThe organization ensures that \\nindividuals with significant \\nsecurity responsibilities \\ncomplete the organization’s \\ndefined specialized security \\ntraining (or comparable \\ntraining for contractors) \\n[within organizationally \\ndefined timeframes] and \\nperiodically thereafter. The \\norganization also maintains \\ncompletion records for \\nspecialized training taken by \\nindividuals with significant \\nsecurity responsibilities.  \\n\\nThe organization obtains \\nfeedback on its security \\ntraining program and uses \\nthat information to make \\nimprovements. \\n\\nThe organization obtains \\nfeedback on its specialized \\nsecurity training content and \\nprocesses and makes updates \\nto its program, as appropriate. \\nIn addition, the organization \\nmeasures the effectiveness of \\nits specialized security \\ntraining program by, for \\nexample, conducting targeted \\nphishing exercises and \\nfollowing up with additional \\ntraining, and/or disciplinary \\naction, as appropriate. \\n\\nThe organization monitors \\nand analyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its security training \\npolicies, procedures, and \\npractices. The organization \\nensures that data supporting \\nmetrics are obtained \\naccurately, consistently, and \\nin a reproducible format. \\n\\nOptimized \\nThe organization has \\ninstitutionalized a process of \\ncontinuous improvement \\nincorporating advanced \\nsecurity training practices and \\ntechnologies. \\n\\nOn a near real-time basis, the \\norganization actively adapts its \\nsecurity training policies, \\nprocedures, processes to a \\nchanging cybersecurity \\nlandscape and provides \\nawareness and training, as \\nappropriate, on evolving and \\nsophisticated threats. \\n\\nQuestion \\n\\n45.  To what extent does the \\norganization ensure that \\nspecialized security training is \\nprovided to individuals with \\nsignificant security \\nresponsibilities (as defined in the \\norganization's security policies \\nand procedures and in \\naccordance with 5 Code of \\nFederal Regulation 930.301) \\n(NIST SP 800-53 REV. 4: AT-3 \\nand AT-4; FY 2021 CIO FISMA \\nMetrics: 2.15, and 5 Code of \\nFederal Regulation 930.301)? \\n\\n46.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s security training \\nprogram that was not noted in \\nthe questions above. Taking into \\nconsideration the maturity level \\ngenerated from the questions \\nabove and based on all testing \\nperformed, is the security \\ntraining program effective? \\n\\nPage 47 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nDetect Function Area (ISCM) \\n\\nDETECT FUNCTION AREA \\nTable 11: ISCM \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented \\n\\nMaturity Level \\n\\nThe organization has not \\ndeveloped, tailored, and \\ncommunicated its ISCM \\npolicies and an \\norganization wide ISCM \\nstrategy. \\n\\n47.  To what extent does the \\norganization utilize  \\ninformation security continuous \\nmonitoring (ISCM) policies \\nand an ISCM strategy that \\naddresses ISCM requirements \\nand activities at each \\norganizational tier (NIST SP \\n800-37 (Rev. 2) Task P-7; \\nNIST SP 800-137: Sections 3.1 \\nand 3.6)? \\n\\nThe organization's ISCM \\npolicies and strategy are \\nconsistently implemented at \\nthe organization, business \\nprocess, and information \\nsystem levels.  \\n\\nIn addition, the strategy \\nsupports clear visibility into \\nassets, awareness into \\nvulnerabilities, up-to-date \\nthreat information, and \\nmission/business impacts.  \\n\\nThe organization also \\nconsistently captures lessons \\nlearned to make \\nimprovements to the ISCM \\npolicies and strategy. \\n\\nThe organization has \\ndeveloped, tailored, and \\ncommunicated its ISCM \\npolicies and strategy. The \\nfollowing areas are included \\n\\n-  Monitoring requirements \\nat each organizational tier \\n-  The minimum monitoring \\n\\nfrequencies for \\nimplemented controls \\nacross the organization. \\nThe criteria for \\ndetermining minimum \\nfrequencies is established \\nin coordination with \\norganizational officials \\n[e.g., senior accountable \\nofficial for risk \\nmanagement, system \\nowners, and common \\ncontrol providers] and in \\naccordance with \\norganizational risk \\ntolerance. \\n\\n-  The organization’s \\nongoing control \\nassessment approach \\n-  How ongoing assessments \\nare to be conducted \\n-  Analyzing ISCM data, \\nreporting findings, and \\nreviewing and updating \\nthe ISCM policies, \\nprocedures, and strategy \\n\\nPage 48 of 60 \\n\\nOptimized \\n\\nThe organization's ISCM \\npolicies and strategy are fully \\nintegrated with its enterprise \\nand supply chain risk \\nmanagement, configuration \\nmanagement, incident \\nresponse, and business \\ncontinuity programs. \\n\\nThe organization can \\ndemonstrate that it is using its \\nISCM policies and strategy to \\nreduce the cost and increase \\nthe efficiency of security and \\nprivacy programs. \\n\\nManaged and \\nMeasurable \\n\\nThe organization monitors \\nand analyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its ISCM policies and \\nstrategy and makes updates, \\nas appropriate. The \\norganization ensures that data \\nsupporting metrics are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nThe organization has \\ntransitioned to ongoing \\ncontrol and system \\nauthorization through the \\nimplementation of its \\ncontinuous monitoring \\npolicies and strategy. \\n\\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n\\x0cManaged and \\nMeasurable \\n\\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to effectively \\nimplement ISCM activities. \\nFurther, stakeholders are held \\naccountable for carrying out \\ntheir roles and \\nresponsibilities effectively. \\n\\nThe organization utilizes the \\nresults of security control \\nassessments and monitoring \\nto maintain ongoing \\nauthorizations of information \\nsystems, including the \\nmaintenance of system \\nsecurity plans. \\n\\nOptimized \\n\\nThe organization's system level \\nISCM policies and strategies \\nare fully integrated with its \\nenterprise and supply chain risk \\nmanagement, configuration \\nmanagement, incident response, \\nand business continuity \\nprograms. \\n\\nThe organization can \\ndemonstrate that it is using its \\nsystem level ISCM policies and \\nstrategy to reduce the cost and \\nincrease the efficiency of \\nsecurity and privacy programs. \\n\\nFY 2021 Inspector General FISMA Metrics v1.1 \\nDetect Function Area (ISCM) \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented \\n\\nMaturity Level \\n\\n48.  To what extent have ISCM \\nstakeholders and their roles, \\nresponsibilities, levels of \\nauthority, and dependencies \\nbeen defined, communicated, \\nand implemented across the \\norganization (NIST SP 800-53 \\nREV. 4: CA-1; NIST SP 800-\\n137; CSF: DE.DP-1;  NIST \\n800-37, Rev. 2 Task P-7 and S-\\n5) \\n\\n49.  How mature are the \\n\\norganization's processes for \\nperforming ongoing information \\nsystem assessments, granting \\nsystem authorizations, including \\ndeveloping and maintaining \\nsystem security plans, and \\nmonitoring system security \\ncontrols (OMB A-130, NIST SP \\n800-137: Section 2.2; NIST SP \\n800-53 REV. 4: CA-2, CA-6, \\nand CA-7; NIST Supplemental \\nGuidance on Ongoing \\nAuthorization; NIST SP 800-37 \\n(Rev. 2) Task S-5; NIST SP \\n800-18, Rev. 1, NIST IR 8011; \\nOMB M-14-03; OMB M-19-\\n03) \\n\\nRoles and responsibilities have \\nnot been fully defined and \\ncommunicated across the \\norganization, including \\nappropriate levels of authority \\nand dependencies. \\n\\nThe organization has \\ndefined and \\ncommunicated the \\nstructures of its ISCM \\nteam, roles and \\nresponsibilities of ISCM \\nstakeholders, and levels of \\nauthority and \\ndependencies. \\n\\nThe organization has not \\ndeveloped system level \\ncontinuous monitoring \\nstrategies/policies that define \\nits processes for performing \\nongoing security control \\nassessments, granting system \\nauthorizations, including \\ndeveloping and maintaining \\nsystem security plans, \\nmonitoring security controls \\nfor individual systems; and \\ntime based triggers for ongoing \\nauthorization. \\n\\nThe organization has \\ndeveloped system level \\ncontinuous monitoring \\nstrategies/policies that define \\nits processes for performing \\nongoing security control \\nassessments, granting system \\nauthorizations, including \\ndeveloping and maintaining \\nsystem security plans, and \\nmonitoring security controls \\nfor individual systems; and \\ntime based triggers for ongoing \\nauthorization. \\n\\nThe system level \\nstrategy/policies address the \\nmonitoring of those controls \\nthat are not addressed by the \\norganizational level strategy, \\nas well as how changes to the \\nsystem are monitored and \\nreported. \\n\\nIndividuals are performing \\nthe roles and responsibilities \\nthat have been defined across \\nthe organization. \\n\\nThe organization consistently \\nimplements its system level \\ncontinuous monitoring \\nstrategies and related \\nprocesses, including \\nperforming ongoing security \\ncontrol assessments, granting \\nsystem authorizations, \\nincluding developing and \\nmaintaining system security \\nplans, and monitoring \\nsecurity controls to provide a \\nview of the organizational \\nsecurity posture, as well as \\neach system’s contribution to \\nsaid security posture. \\n\\nIn conjunction with the \\noverall ISCM strategy, all \\nsecurity control classes \\n(management, operational, \\nand technical) and types \\n(common, hybrid, and \\nsystem-specific) are assessed \\nand monitored, and their \\nstatus updated regularly (as \\ndefined in the agency’s \\ninformation security policy) \\nin security plans. \\n\\nPage 49 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nDetect Function Area (ISCM) \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently Implemented \\n\\nMaturity Level \\n\\nThe organization is \\nconsistently capturing \\nqualitative and quantitative \\nperformance measures on the \\nperformance of its ISCM \\nprogram in accordance with \\nestablished requirements for \\ndata collection, storage, \\nanalysis, retrieval, and \\nreporting. \\n\\nThe organization has not \\nidentified and defined the \\nqualitative and quantitative \\nperformance measures that will \\nbe used to assess the \\neffectiveness of its ISCM \\nprogram, achieve situational \\nawareness, and control \\nongoing risk. Further, the \\norganization has not defined \\nhow ISCM information will be \\nshared with individuals with \\nsignificant security \\nresponsibilities and used to \\nmake risk-based decisions. \\n\\nThe organization has \\nidentified and defined the \\nperformance measures and \\nrequirements that will be \\nused to assess the \\neffectiveness of its ISCM \\nprogram, achieve \\nsituational awareness, and \\ncontrol ongoing risk. In \\naddition, the organization \\nhas defined the format of \\nreports, frequency of \\nreports, and the tools used \\nto provide information to \\nindividuals with \\nsignificant security \\nresponsibilities. \\n\\n50.  How mature is the \\n\\norganization's process for \\ncollecting and analyzing ISCM \\nperformance measures and \\nreporting findings (NIST SP \\n800-137)? \\n\\n51.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s ISCM program \\nthat was not noted in the \\nquestions above. Taking into \\nconsideration the maturity level \\ngenerated from the questions \\nabove and based on all testing \\nperformed, is the ISCM \\nprogram effective? \\n\\nOptimized \\n\\nOn a near real-time basis, the \\norganization actively adapts its \\nISCM program to a changing \\ncybersecurity landscape and \\nresponds to evolving and \\nsophisticated threats in a timely \\nmanner. \\n\\nManaged and \\nMeasurable \\n\\nThe organization is able to \\nintegrate metrics on the \\neffectiveness of its ISCM \\nprogram to deliver persistent \\nsituational awareness across \\nthe organization, explain the \\nenvironment from both a \\nthreat/vulnerability and \\nrisk/impact perspective, and \\ncover mission areas of \\noperations and security \\ndomains. \\n\\nPage 50 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRespond Function Area (Incident Response) \\n\\nRESPOND FUNCTION AREA \\nTable 12: Incident Response \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently I mplemented \\n\\nMaturity Level \\n\\nThe organization consistently \\nimplements its incident \\nresponse plan. Further, the \\norganization is consistently \\ncapturing and  sharing lessons \\nlearned on the effectiveness of \\nits incident response plan and \\nmaking updates as necessary. \\n\\nManaged and \\nMeasurable \\nThe organization monitors and \\nanalyzes the qualitative and \\nquantitative performance \\nmeasures that have been \\ndefined in its incident response \\nplan to monitor and maintain \\nthe effectiveness of its overall \\nincident response capability.  \\nThe organization ensures that \\ndata supporting metrics are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nOptimized \\n\\nThe organization's incident \\nresponse plan is fully integrated \\nwith risk management, \\ncontinuous monitoring, \\ncontinuity of operations, and \\nother mission/business areas, as \\nappropriate. \\n\\nIn addition, the organization \\nmake near real-time updates to \\nits incident response plan based \\non changing risk environments \\nand threat information. \\n\\nThe organization participates in \\nDHS’s Cyber Storm national \\nlevel exercise, as appropriate, \\nor other exercises, to assess, \\ncybersecurity preparedness, and \\nexamine incident response \\nprocesses. \\n\\nIndividuals are performing the \\nroles and responsibilities that \\nhave been defined across the \\norganization.  \\n\\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to effectively \\nimplement incident response \\nactivities. Further, stakeholders \\nare held accountable for \\ncarrying out their roles and \\nresponsibilities effectively. \\n\\nThe organization has not \\ndeveloped an incident \\nresponse plan to provide a \\nroadmap for implementing its \\nincident response capability. \\n\\n52.  To what extent does the \\n\\norganization utilize an incident \\nresponse plan to provide a \\nformal, focused, and coordinated \\napproach to responding to \\nincidents (NIST SP 800-53 \\nREV. 4: IR-8; NIST SP 800-61 \\nRev. 2, section 2.3.2; CSF, \\nRS.RP-1, Presidential Policy \\nDirective (PPD) 8 – National \\nPreparedness)? \\n\\nRoles and responsibilities \\nhave not been fully defined \\nand communicated across the \\norganization, including \\nappropriate levels of authority \\nand dependencies. \\n\\n53.  To what extent have incident \\n\\nresponse team structures/models, \\nstakeholders, and their roles, \\nresponsibilities, levels of \\nauthority, and dependencies \\nbeen defined, communicated, \\nand implemented across the \\norganization (NIST SP 800-53 \\nREV. 4: IR-7; NIST SP 800-83; \\nNIST SP 800-61 Rev. 2; CSF, \\nRS.CO-1, OMB M-20-04; FY \\n2021 CIO FISMA Metrics: \\nSection 4; CSF: RS.CO-1; and \\nUS-CERT Federal Incident \\nNotification Guidelines)? \\n\\nThe organization has \\ndeveloped a tailored incident \\nresponse plan that addresses \\n- \\n\\nStructure and \\norganization of the \\nincident response \\ncapability \\n\\n-  High-level approach for \\n\\nhow the incident \\nresponse capability fits \\ninto the overall \\norganization \\n-  Defines reportable \\nincidents, including \\nmajor incidents \\n-  Metrics for measuring \\nthe incident response \\ncapability \\n-  Resources and \\n\\nmanagement support \\nThe organization has defined \\nand communicated the \\nstructures of its incident \\nresponse teams, roles and \\nresponsibilities of incident \\nresponse stakeholders, and \\nassociated levels of authority \\nand dependencies. In \\naddition, the organization has \\ndesignated a principal \\nsecurity operations center or \\nequivalent organization that is \\naccountable to agency \\nleadership, DHS, and OMB \\nfor all incident response \\nactivities. \\n\\nPage 51 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0cQuestion \\n\\n54.  How mature are the \\n\\norganization's processes for \\nincident detection and analysis? \\n(NIST 800-53: IR-4 and IR-6; \\nNIST SP 800-61 Rev. 2; OMB \\nM-20-04; CSF: DE.AE-1, \\nDE.AE-2 -5, PR.DS-6, RS.AN-1  \\nand 4, and PR.DS-8; and US-\\nCERT Incident Response \\nGuidelines) \\n\\nOptimized \\n\\nFY 2021 Inspector General FISMA Metrics v1.1 \\nRespond Function Area (Incident Response) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently I mplemented \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined and communicated its \\npolicies, procedures, and \\nprocesses for incident \\ndetection and analysis. In \\naddition, the organization has \\nnot defined a common threat \\nvector taxonomy for \\nclassifying incidents and its \\nprocesses for detecting, \\nanalyzing, and prioritizing \\nincidents. \\n\\nThe organization has defined \\nand communicated its \\npolicies, procedures, and \\nprocesses for incident \\ndetection and analysis.  \\n\\nIn addition, the organization \\nhas defined a common threat \\nvector taxonomy and \\ndeveloped handling \\nprocedures for specific types \\nof incidents, as appropriate.  \\n\\nIn addition, the organization \\nhas defined its processes and \\nsupporting technologies for \\ndetecting and analyzing \\nincidents, including the types \\nof precursors and indicators \\nand how they are generated \\nand reviewed, and for \\nprioritizing incidents. \\n\\nThe organization consistently \\nimplements its policies, \\nprocedures, and processes for \\nincident detection and analysis. \\n\\nIn addition, the organization \\nconsistently utilizes its threat \\nvector taxonomy to classify \\nincidents and consistently \\nimplements its processes for \\nincident detection, analysis, \\nand prioritization.  \\n\\nIn addition, the organization \\nconsistently implements, and \\nanalyzes precursors and \\nindicators generated by, for \\nexample, the following \\ntechnologies: intrusion \\ndetection/prevention, security \\ninformation and event \\nmanagement (SIEM), antivirus \\nand antispam software, and file \\nintegrity checking software. \\n\\nFurther, the organization is \\nconsistently capturing and \\nsharing lessons learned on the \\neffectiveness of  its incident \\ndetection policies and \\nprocedures and making updates \\nas necessary. \\n\\nManaged and \\nMeasurable \\nThe organization monitors and \\nanalyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its incident detection and \\nanalysis policies and \\nprocedures. The organization \\nensures that data supporting \\nmetrics are obtained \\naccurately, consistently, and in \\na reproducible format. \\n\\nThe organization utilizes \\nprofiling techniques to measure \\nthe characteristics of expected \\nactivities on its networks and \\nsystems so that it can more \\neffectively detect security \\nincidents. Examples of \\nprofiling include running file \\nintegrity checking software on \\nhosts to derive checksums for \\ncritical files and monitoring \\nnetwork bandwidth usage to \\ndetermine what the average \\nand peak usage levels are on \\nvarious days and times. \\nThrough profiling techniques, \\nthe organization maintains a \\ncomprehensive baseline of \\nnetwork operations and \\nexpected data flows for users \\nand systems. \\n\\nPage 52 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRespond Function Area (Incident Response) \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently I mplemented \\n\\nMaturity Level \\n\\n55.  How mature are the \\n\\norganization's processes for \\nincident handling (NIST 800-53: \\nIR-4; NIST SP 800-61, Rev. 2; \\nCSF: RS.MI-1 and 2) \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes \\nfor incident handling to \\ninclude containment \\nstrategies for various types \\nof major incidents, \\neradication activities to \\neliminate components of an \\nincident and mitigate any \\nvulnerabilities that were \\nexploited, and recovery of \\nsystems. \\n\\n56.  To what extent does the \\n\\norganization ensure that incident \\nresponse information is shared \\nwith individuals with significant \\nsecurity responsibilities and \\nreported to external stakeholders \\nin a timely manner (FISMA; \\nOMB M-20-04; NIST SP 800-53 \\nREV. 4: IR-6; US-CERT \\nIncident Notification Guidelines; \\nPPD-41; CSF: RS.CO-2 through \\n5; DHS Cyber Incident \\nReporting Unified Message) \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes to \\nshare incident response \\ninformation with individuals \\nwith significant security \\nresponsibilities or its \\nprocesses for reporting \\nsecurity incidents, including \\nmajor incidents, to US-\\nCERT and other \\nstakeholders (e.g., Congress \\nand the Inspector General, as \\napplicable) in a timely \\nmanner. \\n\\nThe organization has defined \\nits policies, procedures, and \\nprocesses for incident handling \\nto include containment \\nstrategies for each key incident \\ntype. In developing its \\nstrategies, the organization \\ntakes into consideration: the \\npotential damage to and theft \\nof resources, the need for \\nevidence preservation, service \\navailability, time and resources \\nneeded to implement the \\nstrategy, effectiveness of the \\nstrategy, and duration of the \\nsolution. In addition, the \\norganization has defined its \\nprocesses to eradicate \\ncomponents of an incident, \\nmitigate any vulnerabilities \\nthat were exploited, and \\nrecover system operations. \\nThe organization has defined \\nits policies, procedures, and \\nprocesses to report suspected \\nsecurity incidents to the \\norganization's incident \\nresponse capability within \\norganization defined \\ntimeframes. In addition, the \\norganization has defined its \\nprocesses for reporting \\nsecurity incident information, \\nincluding for major incidents, \\nto US-CERT, law \\nenforcement, the Congress and \\nthe Office of Inspector \\nGeneral, as appropriate. \\n\\nThe organization consistently \\nimplements its incident \\nhandling policies, procedures, \\ncontainment strategies, and \\nincident eradication processes. \\n\\nIn addition, the organization \\nconsistently implements \\nprocesses to remediate \\nvulnerabilities that may have \\nbeen exploited on the target \\nsystem(s), and recovers  \\nsystem operations. \\n\\nFurther, the organization is \\nconsistently capturing and \\nsharing lessons learned on the \\neffectiveness of  its incident \\nhandling policies and \\nprocedures and making updates \\nas necessary. \\n\\nThe organization consistently \\nshares information on incident \\nactivities with internal \\nstakeholders. The organization \\nensures that security incidents \\nare reported to US-CERT, law \\nenforcement, the Office of \\nInspector General, and the \\nCongress (for major incidents) \\nin a timely manner. \\n\\nFurther, the organization is \\nconsistently capturing and \\nsharing lessons learned on the \\neffectiveness of  its incident \\nreporting policies and \\nprocedures and making \\nupdates as necessary. \\n\\nPage 53 of 60 \\n\\nOptimized \\n\\nThe organization utilizes \\ndynamic reconfiguration \\n(e.g., changes to router rules, \\naccess control lists, and filter \\nrules for firewalls and \\ngateways) to stop attacks, \\nmisdirect attackers, and to \\nisolate components of \\nsystems. \\n\\nThe organization receives, \\nretains, uses, and \\ndisseminates cyber threat \\nindicators in accordance with \\nthe Cybersecurity Information \\nSharing Act of 2015.  \\n\\nManaged and \\nMeasurable \\nThe organization monitors and \\nanalyzes qualitative and \\nquantitative performance \\nmeasures on the effectiveness \\nof its incident handling policies \\nand procedures. The \\norganization ensures that data \\nsupporting metrics are obtained \\naccurately, consistently, and in \\na reproducible format. \\n\\nThe organization manages and \\nmeasures the impact of \\nsuccessful incidents and can \\nquickly mitigate related \\nvulnerabilities on other \\nsystems so that they are not \\nsubject to exploitation of the \\nsame vulnerability. \\n\\nIncident response metrics are \\nused to measure and manage \\nthe timely reporting of \\nincident information to \\norganizational officials and \\nexternal stakeholders. The \\norganization ensures that data \\nsupporting metrics are \\nobtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRespond Function Area (Incident Response) \\n\\nQuestion \\n\\n57.  To what extent does the \\n\\norganization collaborate with \\nstakeholders to ensure on-site, \\ntechnical assistance/surge \\ncapabilities can be leveraged for \\nquickly responding to incidents, \\nincluding through \\ncontracts/agreements, as \\nappropriate, for incident \\nresponse support (NIST SP 800-\\n86; NIST SP 800-53 REV. 4: IR-\\n4; OMB M-20-04; PPD-41). \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently I mplemented \\n\\nMaturity Level \\n\\nThe organization has not \\ndefined how it will \\ncollaborate with DHS and \\nother parties, as appropriate, \\nto provide on-site, technical \\nassistance/surge \\nresources/special capabilities \\nfor quickly responding to \\nincidents. In addition, the \\norganization has not defined \\nhow it plans to utilize DHS' \\nEinstein program for \\nintrusion \\ndetection/prevention \\ncapabilities for traffic \\nentering and leaving the \\norganization's networks. \\n\\nThe organization has defined \\nhow it will collaborate with \\nDHS and other parties, as \\nappropriate, to provide on-site, \\ntechnical assistance/surge \\nresources/special capabilities \\nfor quickly responding to \\nincidents. This includes \\nidentification of incident \\nresponse services that may \\nneed to be procured to support \\norganizational processes. In \\naddition, the organization has \\ndefined how it plans to utilize \\nDHS' Einstein program for \\nintrusion detection/prevention \\ncapabilities for traffic entering \\nand leaving the organization's \\nnetworks. \\n\\nThe organization consistently \\nutilizes on-site, technical \\nassistance/surge capabilities \\noffered by DHS or ensures \\nthat such capabilities are in \\nplace and can be leveraged \\nwhen needed. In addition, the \\norganization has entered into \\ncontractual relationships in \\nsupport of incident response \\nprocesses (e.g., for forensic \\nsupport), as needed. The \\norganization has fully \\ndeployed DHS’ Einstein 1 and \\n2 to screen all traffic entering \\nand leaving its network \\nthrough a TIC. \\n\\nOptimized \\n\\nManaged and \\nMeasurable \\n\\nThe organization utilizes \\nEinstein 3 Accelerated, and/or \\nother comparable tools or \\nservices, to detect and \\nproactively block cyber-\\nattacks or prevent potential \\ncompromises. \\n\\nPage 54 of 60 \\n\\n \\n \\n \\n\\x0cManaged and \\nMeasurable \\nThe organization evaluates the \\neffectiveness of its incident \\nresponse technologies and \\nmakes adjustments to \\nconfigurations and toolsets, as \\nappropriate. \\n\\nOptimized \\n\\nThe organization has \\ninstitutionalized the \\nimplementation of advanced \\nincident response \\ntechnologies for analysis of \\ntrends and performance \\nagainst benchmarks (e.g., \\nsimulation based technologies \\nto continuously determine the \\nimpact of potential security \\nincidents to its IT assets) and \\nadjusts incident response \\nprocesses and security \\nmeasures accordingly. \\n\\nFY 2021 Inspector General FISMA Metrics v1.1 \\nRespond Function Area (Incident Response) \\n\\nQuestion \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently I mplemented \\n\\nMaturity Level \\n\\nThe organization has not \\nidentified and defined its \\nrequirements for incident \\nresponse technologies \\nneeded in one or more of the \\nspecified areas and relies on \\nmanual/procedural methods \\nin instances where \\nautomation would be more \\neffective.  \\n\\nThe organization has identified \\nand fully defined its \\nrequirements for the incident \\nresponse technologies it plans \\nto utilize in the specified areas. \\nWhile tools are implemented \\nto support some incident \\nresponse activities, the tools \\nare not interoperable to the \\nextent practicable, do not \\ncover all components of the \\norganization’s network, and/or \\nhave not been configured to \\ncollect and retain relevant and \\nmeaningful data consistent \\nwith the organization’s \\nincident response policy, \\nplans, and procedures. \\n\\nThe organization has \\nconsistently implemented its \\ndefined incident response \\ntechnologies in the specified \\nareas. In addition, the \\ntechnologies utilized are \\ninteroperable to the extent \\npracticable, cover all \\ncomponents of the \\norganization's network, and \\nhave been configured to \\ncollect and retain relevant and \\nmeaningful data consistent \\nwith the organization’s \\nincident response policy, \\nprocedures, and plans. \\n\\n58.  To what extent does the \\n\\norganization utilize the following \\ntechnology to support its incident \\nresponse program? \\n\\n-Web application \\nprotections, such as web \\napplication firewalls \\n-Event and incident management, \\nsuch as intrusion detection and \\nprevention tools, and incident \\ntracking and reporting tools \\n-Aggregation and analysis, \\nsuch as security information \\nand event management (SIEM) \\nproducts \\n-Malware detection, such as \\nantivirus and antispam software \\ntechnologies \\n- Information management, such \\nas data loss prevention \\n- File integrity and endpoint and \\nserver security tools (NIST SP \\n800-137; NIST SP 800-61, Rev. \\n2; NIST SP 800-44) \\n\\n59.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s incident response \\nprogram that was not noted in the \\nquestions above. Taking into \\nconsideration the maturity level \\ngenerated from the questions \\nabove and based on all testing \\nperformed, is the incident \\nresponse program effective? \\n\\nPage 55 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRecover Function Area (Contingency Planning) \\n\\nRECOVER FUNCTION AREA \\nTable 13: Contingency Planning \\n\\nQuestion \\n\\n60.  To what extent have roles and \\nresponsibilities of stakeholders \\ninvolved in information systems \\ncontingency planning been \\ndefined, communicated, and \\nimplemented across the \\norganization, including \\nappropriate delegations of \\nauthority (NIST SP 800-53 \\nREV. 4: CP-1, CP-2, and CP-3; \\nNIST SP 800-34; NIST SP 800-\\n84; FCD-1: Annex B)? \\n\\nAd Hoc \\n\\nDefined \\n\\nRoles and responsibilities \\nhave not been fully defined \\nand communicated across the \\norganization, including \\nappropriate delegations of \\nauthority. \\n\\nRoles and responsibilities of \\nstakeholders have been fully \\ndefined and communicated \\nacross the organization, \\nincluding appropriate \\ndelegations of authority. In \\naddition, the organization has \\ndesignated appropriate teams \\nto implement its contingency \\nplanning strategies. Further, \\nthe organization has defined \\nits policies and procedures for \\nproviding contingency \\ntraining consistent with roles \\nand responsibilities. \\n\\nMaturity Level \\n\\nIndividuals are performing the \\nroles and responsibilities that \\nhave been defined across the \\norganization.  \\n\\nConsistently  Implemented  Managed and Measurable \\nResources (people, processes, \\nand technology) are allocated \\nin a risk-based manner for \\nstakeholders to effectively \\nimplement system contingency \\nplanning activities. Further, \\nstakeholders are held \\naccountable for carrying out \\ntheir roles and responsibilities \\neffectively. \\n\\nThe organization ensures that \\ncontingency training is \\nprovided consistent with roles \\nand responsibilities to ensure \\nthat the appropriate content \\nand level of detail is included. \\n\\nOptimized \\nThe organization incorporates \\nsimulated events into \\ncontingency training to \\nfacilitate effective response by \\nstakeholders (internal and \\nexternal) involved in \\ninformation systems \\ncontingency planning and to \\nmeasure the extent to which \\nindividuals are equipped to \\nperform their roles and \\nresponsibilities. \\n\\nPage 56 of 60 \\n\\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRecover Function Area (Contingency Planning) \\n\\nQuestion \\n\\n61.  To what extent does the \\n\\norganization ensure that the \\nresults of business impact \\nanalyses (BIA) are used to guide \\ncontingency planning efforts \\n(NIST SP 800-53 REV. 4: CP-2; \\nNIST SP 800-34, Rev. 1, 3.2; \\nNIST IR 8286;  FIPS 199; FCD-\\n1; OMB M-19-03; FY 2021 CIO \\nFISMA Metrics, Section 5; \\nCSF:ID.RA-4)? \\n\\nAd Hoc \\n\\nDefined \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes \\nfor conducting \\norganizational and system-\\nlevel BIAs and for \\nincorporating the results into \\nstrategy and plan \\ndevelopment efforts. \\n\\nThe organization has defined \\nits policies, procedures, and \\nprocesses for conducting \\norganizational and system-\\nlevel BIAs and for \\nincorporating the results into \\nstrategy and plan development \\nefforts. \\n\\nMaturity Level \\n\\nThe organization consistently \\nincorporates the results of \\norganizational and system \\nlevel BIAs into strategy and \\nplan development efforts. \\n\\nConsistently  Implemented  Managed and Measurable \\nThe organization ensures that \\nthe results of organizational \\nand system level BIA’s are \\nintegrated with enterprise risk \\nmanagement processes, for \\nconsistently evaluating, \\nrecording, and monitoring the \\ncriticality and sensitivity of \\nenterprise assets. \\n\\nOptimized \\n\\nSystem level BIAs are \\nintegrated with the \\norganizational level BIA and \\ninclude: characterization of all \\nsystem components, \\ndetermination of \\nmissions/business processes \\nand recovery criticality, \\nidentification of resource \\nrequirements, and \\nidentification of recovery \\npriorities for system resources. \\nThe results of the BIA are \\nconsistently used to determine \\ncontingency planning \\nrequirements and priorities, \\nincluding mission essential \\nfunctions/high value assets. \\n\\nAs appropriate, the \\norganization utilizes the \\nresults of its BIA in \\nconjunction with its risk \\nregister to calculate potential \\nlosses and inform senior level \\ndecision making.  \\n\\nPage 57 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRecover Function Area (Contingency Planning) \\n\\nAd Hoc \\n\\nDefined \\n\\nConsistently  Implemented  Managed and Measurable \\n\\nOptimized \\n\\nMaturity Level \\n\\nQuestion \\n\\n62.  To what extent does the \\norganization ensure that \\ninformation system contingency \\nplans are developed, maintained, \\nand integrated with other \\ncontinuity plans (NIST SP 800-\\n53 REV. 4: CP-2; NIST SP 800-\\n34; FY 2021 CIO FISMA \\nMetrics: 5.1; OMB M-19-03; \\nCSF: PR.IP-9)? \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes \\nfor information system \\ncontingency plan (ISCP) \\ndevelopment and \\nmaintenance. In addition, the \\norganization has not \\ndeveloped templates to guide \\nplan development; and \\nsystem contingency plans \\nare developed in an ad-hoc \\nmanner with limited \\nintegration with other \\ncontinuity plans. \\n\\nThe organization has defined \\nits policies, procedure, and \\nprocesses for information \\nsystem contingency plan \\ndevelopment, maintenance, \\nand integration with other \\ncontinuity areas.  \\n\\nThe policies, procedures, and \\nprocesses for ISCP include the \\nfollowing phases: activation \\nand notification, recovery, and \\nreconstitution. \\n\\n63.  To what extent does the \\n\\norganization perform \\ntests/exercises of its information \\nsystem contingency planning \\nprocesses (NIST SP 800-34; \\nNIST SP 800-53 REV. 4: CP-3 \\nand CP-4; FY 2021 CIO FISMA \\nMetrics, Section 5; CSF: ID.SC-\\n5 and CSF: PR.IP-10)? \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, and processes \\nfor information system \\ncontingency plan \\ntesting/exercises. ISCP tests \\nare performed in an ad-hoc, \\nreactive manner. \\n\\nPolicies, procedures, and \\nprocesses for information \\nsystem contingency plan \\ntesting and exercises have been \\ndefined and include, as \\napplicable, notification \\nprocedures, system recovery \\non an alternate platform from \\nbackup media, internal and \\nexternal connectivity, system \\nperformance using alternate \\nequipment, restoration of \\nnormal procedures, and \\ncoordination with other \\nbusiness areas/continuity \\nplans, and tabletop and \\nfunctional exercises. \\n\\nInformation system \\ncontingency plans are \\nconsistently developed and \\nimplemented for systems, as \\nappropriate, and include \\norganizational and system \\nlevel considerations for the \\nfollowing phases: activation \\nand notification, recovery, and \\nreconstitution. \\n\\nIn addition, system level \\ncontingency planning \\ndevelopment/maintenance \\nactivities are integrated with \\nother continuity areas \\nincluding organization and \\nbusiness process continuity, \\ndisaster recovery planning, \\nincident management, insider \\nthreat implementation plan (as \\nappropriate), and occupant \\nemergency plans. \\nInformation system \\ncontingency plan testing and \\nexercises are consistently \\nimplemented. ISCP testing \\nand exercises are integrated, to \\nthe extent practicable, with \\ntesting of related plans, such \\nas incident response \\nplan/COOP/BCP. \\n\\nInformation system \\ncontingency planning \\nactivities are fully integrated \\nwith the enterprise risk \\nmanagement program, \\nstrategic planning processes, \\ncapital allocation/budgeting, \\nand other mission/business \\nareas and embedded into \\ndaily decision making across \\nthe organization. \\n\\nThe organization is able to \\nintegrate metrics on the \\neffectiveness of its \\ninformation system \\ncontingency plans with \\ninformation on the \\neffectiveness of related plans, \\nsuch as organization and \\nbusiness process continuity, \\ndisaster recovery, incident \\nmanagement, insider threat \\nimplementation, and occupant \\nemergency, as appropriate to \\ndeliver persistent situational \\nawareness across the \\norganization. \\n\\nThe organization coordinates \\nthe development of ISCP’s \\nwith the contingency plans of \\nexternal service providers. \\n\\nThe organization employs \\nautomated mechanisms to test \\nsystem contingency plans \\nmore thoroughly and \\neffectively. \\n\\nBased on risk, the \\norganization performs a full \\nrecovery and reconstitution of \\nsystems to a known state. \\n\\nIn addition, the organization \\ncoordinates plan testing with \\nexternal stakeholders (e.g., \\nICT supply chain \\npartners/providers), as \\nappropriate. \\n\\nIn addition, the organization \\nproactively employs \\n[organization defined \\nmechanisms] to disrupt or \\nadversely affect the system or \\nsystem component and test \\nthe effectiveness of \\ncontingency planning \\nprocesses. \\n\\nPage 58 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRecover Function Area (Contingency Planning) \\n\\nQuestion \\n\\n64.  To what extent does the \\n\\norganization perform \\ninformation system backup and \\nstorage, including use of \\nalternate storage and processing \\nsites, as appropriate (NIST SP \\n800-53 REV. 4: CP-6, CP-7, CP-\\n8, and CP-9; NIST SP 800-34: \\n3.4.1, 3.4.2, 3.4.3; FCD-1; NIST \\nCSF: PR.IP-4; FY 2021 CIO \\nFISMA Metrics, Section 5; and \\nNARA guidance on information \\nsystems security records)? \\n\\nAd Hoc \\n\\nDefined \\n\\nThe organization has not \\ndefined its policies, \\nprocedures, processes, \\nstrategies, and technologies \\nfor information system \\nbackup and storage, \\nincluding the use of alternate \\nstorage and processing sites \\nand redundant array of \\nindependent disks (RAID), \\nas appropriate. Information \\nsystem backup and storage is \\nperformed in an ad- hoc, \\nreactive manner. \\n\\nThe organization has defined \\nits policies, procedures, \\nprocesses, strategies, and \\ntechnologies for information \\nsystem backup and storage, \\nincluding use of alternate \\nstorage and processing sites \\nand RAID, as appropriate. \\n\\nThe organization has \\nconsidered alternative \\napproaches when developing \\nits backup and storage \\nstrategies, including cost, \\nenvironment (e.g., cloud model \\ndeployed), maximum \\ndowntimes, recovery priorities, \\nand integration with other \\ncontingency plans. \\n\\nOptimized \\n\\nMaturity Level \\n\\nThe organization consistently \\nimplements its policies, \\nprocedures, processes, \\nstrategies, and technologies \\nfor information system backup \\nand storage, including the use \\nof alternate storage and \\nprocessing sites and RAID, as \\nappropriate.  \\n\\nConsistently  Implemented  Managed and Measurable \\nThe organization ensures that \\nits information system backup \\nand storage processes, \\nincluding use of alternate \\nstorage and processing sties, \\nand related supply chain \\ncontrols, are assessed, as \\nappropriate, as part of its \\ncontinuous monitoring \\nprogram.  \\n\\nAs part of its continuous \\nmonitoring processes, the \\norganization demonstrates that \\nits system backup and storage \\nand alternate storage and \\nprocessing sites are \\nconfigured to facilitate \\nrecovery operations in \\naccordance with recovery time \\nand recover point objectives. \\n\\nAlternate processing and \\nstorage sites are chosen based \\nupon risk assessments that \\nensure the potential disruption \\nof the organization’s ability to \\ninitiate and sustain operations \\nis minimized. In addition, the \\norganization ensures that these \\nsites and are not subject to the \\nsame risks as the primary site.  \\n\\nFurthermore, the organization \\nensures that alternate \\nprocessing and storage \\nfacilities are configured with \\ninformation security \\nsafeguards equivalent to those \\nof the primary site, including \\napplicable ICT supply chain \\ncontrols. Furthermore, \\nbackups of information at the \\nuser- and system-levels are \\nconsistently performed, and \\nthe confidentiality, integrity, \\nand availability of this \\ninformation is maintained. \\n\\nPage 59 of 60 \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cFY 2021 Inspector General FISMA Metrics v1.1 \\nRecover Function Area (Contingency Planning) \\n\\nAd Hoc \\n\\nDefined \\n\\nThe organization has not \\ndefined how the planning \\nand performance of recovery \\nactivities are communicated \\nto internal stakeholders and \\nexecutive management \\nteams and used to make risk-\\nbased decisions. \\n\\nThe organization has defined \\nhow the planning and \\nperformance of recovery \\nactivities are communicated to \\ninternal stakeholders and \\nexecutive management teams. \\n\\nMaturity Level \\n\\nInformation on the planning \\nand performance of recovery \\nactivities is consistently \\ncommunicated to relevant \\nstakeholders and executive \\nmanagement teams, who \\nutilize the information to make \\nrisk-based decisions. \\n\\nConsistently  Implemented  Managed and Measurable \\nMetrics on the effectiveness of \\nrecovery activities are \\ncommunicated to relevant \\nstakeholders and the \\norganization has ensured that \\nthe data supporting the metrics \\nare obtained accurately, \\nconsistently, and in a \\nreproducible format. \\n\\nOptimized \\n\\nQuestion \\n\\n65.  To what level does the \\n\\norganization ensure that \\ninformation on the planning and \\nperformance of recovery \\nactivities is communicated to \\ninternal stakeholders and \\nexecutive management teams \\nand used to make risk based \\ndecisions (CSF: RC.CO-3; NIST \\nSP 800-53 REV. 4: CP-2 and IR-\\n4)? \\n\\n66.  Provide any additional \\n\\ninformation on the effectiveness \\n(positive or negative) of the \\norganization’s contingency \\nplanning program that was not \\nnoted in the questions above. \\nTaking into consideration the \\nmaturity level generated from \\nthe questions above and based \\non all testing performed, is the \\ncontingency program effective? \\n\\nPage 60 of 60 \\n\\n \\n \\n \\n \\n \\n\\x0c\"\n"
     ]
    }
   ],
   "source": [
    "text = extract_text(r'C:\\Users\\Tim\\Downloads\\IGHELP.pdf')\n",
    "print(repr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bcb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
